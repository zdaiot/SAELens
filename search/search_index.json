{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#saelens","title":"SAELens","text":"<p>The SAELens training codebase exists to help researchers:</p> <ul> <li>Train sparse autoencoders.</li> <li>Analyse sparse autoencoders and neural network internals.</li> <li>Generate insights which make it easier to create safe and aligned AI systems.</li> </ul> <p>Please note these docs are in beta. We intend to make them cleaner and more comprehensive over time.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install sae-lens\n</code></pre>"},{"location":"#loading-sparse-autoencoders-from-huggingface","title":"Loading Sparse Autoencoders from Huggingface","text":"<p>To load a pretrained sparse autoencoder, you can use <code>SAE.from_pretrained()</code> as below. Note that we return the original cfg dict from the huggingface repo so that it's easy to debug older configs that are being handled when we import an SAe. We also return a sparsity tensor if it is present in the repo. For an example repo structure, see here. </p> <pre><code>from sae_lens import SAE\n\nsae, cfg_dict, sparsity = SAE.from_pretrained(\n    release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n    sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n    device = device\n)\n</code></pre> <p>You can see other importable SAEs in   <code>sae_lens/pretrained_saes.yaml</code>.</p> <p>(We'd accept a PR that converts this yaml to a nice table in the docs!)</p>"},{"location":"#background-and-further-readings","title":"Background and further Readings","text":"<p>We highly recommend this tutorial.</p> <p>For recent progress in SAEs, we recommend the LessWrong forum's Sparse Autoencoder tag</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>I wrote a tutorial to show users how to do some basic exploration of their SAE:</p> <ul> <li>Loading and Analysing Pre-Trained Sparse Autoencoders </li> <li>Understanding SAE Features with the Logit Lens </li> <li>Training a Sparse Autoencoder </li> </ul>"},{"location":"#example-wandb-dashboard","title":"Example WandB Dashboard","text":"<p>WandB Dashboards provide lots of useful insights while training SAE's. Here's a screenshot from one training run. </p> <p></p>"},{"location":"#citation","title":"Citation","text":"<pre><code>@misc{bloom2024saetrainingcodebase,\n   title = {SAELens Training\n   author = {Joseph Bloom, David Channin},\n   year = {2024},\n   howpublished = {\\url{}},\n}}\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#sae_lens.ActivationsStore","title":"<code>ActivationsStore</code>","text":"<p>Class for streaming tokens and generating and storing activations while training SAEs.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>class ActivationsStore:\n    \"\"\"\n    Class for streaming tokens and generating and storing activations\n    while training SAEs.\n    \"\"\"\n\n    model: HookedRootModule\n    dataset: HfDataset\n    cached_activations_path: str | None\n    tokens_column: Literal[\"tokens\", \"input_ids\", \"text\"]\n    hook_name: str\n    hook_layer: int\n    hook_head_index: int | None\n    _dataloader: Iterator[Any] | None = None\n    _storage_buffer: torch.Tensor | None = None\n    device: torch.device\n\n    @classmethod\n    def from_config(\n        cls,\n        model: HookedRootModule,\n        cfg: LanguageModelSAERunnerConfig | CacheActivationsRunnerConfig,\n        dataset: HfDataset | None = None,\n    ) -&gt; \"ActivationsStore\":\n        cached_activations_path = cfg.cached_activations_path\n        # set cached_activations_path to None if we're not using cached activations\n        if (\n            isinstance(cfg, LanguageModelSAERunnerConfig)\n            and not cfg.use_cached_activations\n        ):\n            cached_activations_path = None\n        return cls(\n            model=model,\n            dataset=dataset or cfg.dataset_path,\n            streaming=cfg.streaming,\n            hook_name=cfg.hook_name,\n            hook_layer=cfg.hook_layer,\n            hook_head_index=cfg.hook_head_index,\n            context_size=cfg.context_size,\n            d_in=cfg.d_in,\n            n_batches_in_buffer=cfg.n_batches_in_buffer,\n            total_training_tokens=cfg.training_tokens,\n            store_batch_size_prompts=cfg.store_batch_size_prompts,\n            train_batch_size_tokens=cfg.train_batch_size_tokens,\n            prepend_bos=cfg.prepend_bos,\n            normalize_activations=cfg.normalize_activations,\n            device=torch.device(cfg.act_store_device),\n            dtype=cfg.dtype,\n            cached_activations_path=cached_activations_path,\n            model_kwargs=cfg.model_kwargs,\n            autocast_lm=cfg.autocast_lm,\n            dataset_trust_remote_code=cfg.dataset_trust_remote_code,\n        )\n\n    @classmethod\n    def from_sae(\n        cls,\n        model: HookedRootModule,\n        sae: SAE,\n        streaming: bool = True,\n        store_batch_size_prompts: int = 8,\n        n_batches_in_buffer: int = 8,\n        train_batch_size_tokens: int = 4096,\n        total_tokens: int = 10**9,\n        device: str = \"cpu\",\n    ) -&gt; \"ActivationsStore\":\n\n        return cls(\n            model=model,\n            dataset=sae.cfg.dataset_path,\n            d_in=sae.cfg.d_in,\n            hook_name=sae.cfg.hook_name,\n            hook_layer=sae.cfg.hook_layer,\n            hook_head_index=sae.cfg.hook_head_index,\n            context_size=sae.cfg.context_size,\n            prepend_bos=sae.cfg.prepend_bos,\n            streaming=streaming,\n            store_batch_size_prompts=store_batch_size_prompts,\n            train_batch_size_tokens=train_batch_size_tokens,\n            n_batches_in_buffer=n_batches_in_buffer,\n            total_training_tokens=total_tokens,\n            normalize_activations=sae.cfg.normalize_activations,\n            dataset_trust_remote_code=sae.cfg.dataset_trust_remote_code,\n            dtype=sae.cfg.dtype,\n            device=torch.device(device),\n        )\n\n    def __init__(\n        self,\n        model: HookedRootModule,\n        dataset: HfDataset | str,\n        streaming: bool,\n        hook_name: str,\n        hook_layer: int,\n        hook_head_index: int | None,\n        context_size: int,\n        d_in: int,\n        n_batches_in_buffer: int,\n        total_training_tokens: int,\n        store_batch_size_prompts: int,\n        train_batch_size_tokens: int,\n        prepend_bos: bool,\n        normalize_activations: str,\n        device: torch.device,\n        dtype: str,\n        cached_activations_path: str | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        autocast_lm: bool = False,\n        dataset_trust_remote_code: bool | None = None,\n    ):\n        self.model = model\n        if model_kwargs is None:\n            model_kwargs = {}\n        self.model_kwargs = model_kwargs\n        self.dataset = (\n            load_dataset(\n                dataset,\n                split=\"train\",\n                streaming=streaming,\n                trust_remote_code=dataset_trust_remote_code,  # type: ignore\n            )\n            if isinstance(dataset, str)\n            else dataset\n        )\n        self.hook_name = hook_name\n        self.hook_layer = hook_layer\n        self.hook_head_index = hook_head_index\n        self.context_size = context_size\n        self.d_in = d_in\n        self.n_batches_in_buffer = n_batches_in_buffer\n        self.total_training_tokens = total_training_tokens\n        self.store_batch_size_prompts = store_batch_size_prompts\n        self.train_batch_size_tokens = train_batch_size_tokens\n        self.prepend_bos = prepend_bos\n        self.normalize_activations = normalize_activations\n        self.device = torch.device(device)\n        self.dtype = DTYPE_MAP[dtype]\n        self.cached_activations_path = cached_activations_path\n        self.autocast_lm = autocast_lm\n\n        self.n_dataset_processed = 0\n        self.iterable_dataset = iter(self.dataset)\n\n        self.estimated_norm_scaling_factor = 1.0\n\n        # Check if dataset is tokenized\n        dataset_sample = next(self.iterable_dataset)\n\n        # check if it's tokenized\n        if \"tokens\" in dataset_sample.keys():\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"tokens\"\n        elif \"input_ids\" in dataset_sample.keys():\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"input_ids\"\n        elif \"text\" in dataset_sample.keys():\n            self.is_dataset_tokenized = False\n            self.tokens_column = \"text\"\n        else:\n            raise ValueError(\n                \"Dataset must have a 'tokens', 'input_ids', or 'text' column.\"\n            )\n        self.iterable_dataset = iter(self.dataset)  # Reset iterator after checking\n\n        self.check_cached_activations_against_config()\n\n        # TODO add support for \"mixed loading\" (ie use cache until you run out, then switch over to streaming from HF)\n\n    def check_cached_activations_against_config(self):\n\n        if self.cached_activations_path is not None:  # EDIT: load from multi-layer acts\n            assert self.cached_activations_path is not None  # keep pyright happy\n            # Sanity check: does the cache directory exist?\n            assert os.path.exists(\n                self.cached_activations_path\n            ), f\"Cache directory {self.cached_activations_path} does not exist. Consider double-checking your dataset, model, and hook names.\"\n\n            self.next_cache_idx = 0  # which file to open next\n            self.next_idx_within_buffer = 0  # where to start reading from in that file\n\n            # Check that we have enough data on disk\n            first_buffer = self.load_buffer(\n                f\"{self.cached_activations_path}/{self.next_cache_idx}.safetensors\"\n            )\n\n            buffer_size_on_disk = first_buffer.shape[0]\n            n_buffers_on_disk = len(os.listdir(self.cached_activations_path))\n\n            # Note: we're assuming all files have the same number of tokens\n            # (which seems reasonable imo since that's what our script does)\n            n_activations_on_disk = buffer_size_on_disk * n_buffers_on_disk\n            assert (\n                n_activations_on_disk &gt;= self.total_training_tokens\n            ), f\"Only {n_activations_on_disk/1e6:.1f}M activations on disk, but total_training_tokens is {self.total_training_tokens/1e6:.1f}M.\"\n\n    def apply_norm_scaling_factor(self, activations: torch.Tensor) -&gt; torch.Tensor:\n        return activations * self.estimated_norm_scaling_factor\n\n    def unscale(self, activations: torch.Tensor) -&gt; torch.Tensor:\n        return activations / self.estimated_norm_scaling_factor\n\n    def get_norm_scaling_factor(self, activations: torch.Tensor) -&gt; torch.Tensor:\n        return (self.d_in**0.5) / activations.norm(dim=-1).mean()\n\n    @torch.no_grad()\n    def estimate_norm_scaling_factor(self, n_batches_for_norm_estimate: int = int(1e3)):\n\n        norms_per_batch = []\n        for _ in tqdm(\n            range(n_batches_for_norm_estimate), desc=\"Estimating norm scaling factor\"\n        ):\n            acts = self.next_batch()\n            norms_per_batch.append(acts.norm(dim=-1).mean().item())\n        mean_norm = np.mean(norms_per_batch)\n        scaling_factor = np.sqrt(self.d_in) / mean_norm\n\n        return scaling_factor\n\n    @property\n    def storage_buffer(self) -&gt; torch.Tensor:\n        if self._storage_buffer is None:\n            self._storage_buffer = self.get_buffer(self.n_batches_in_buffer // 2)\n\n        return self._storage_buffer\n\n    @property\n    def dataloader(self) -&gt; Iterator[Any]:\n        if self._dataloader is None:\n            self._dataloader = self.get_data_loader()\n        return self._dataloader\n\n    def get_batch_tokens(self, batch_size: int | None = None):\n        \"\"\"\n        Streams a batch of tokens from a dataset.\n        \"\"\"\n        if not batch_size:\n            batch_size = self.store_batch_size_prompts\n        context_size = self.context_size\n        device = self.device\n\n        batch_tokens = torch.zeros(\n            size=(0, context_size), device=device, dtype=torch.long, requires_grad=False\n        )\n\n        current_batch = []\n        current_length = 0\n\n        while batch_tokens.shape[0] &lt; batch_size:\n            tokens = self._get_next_dataset_tokens()\n            token_len = tokens.shape[0]\n\n            # TODO: Fix this so that we are limiting how many tokens we get from the same context.\n            assert self.model.tokenizer is not None  # keep pyright happy\n            while token_len &gt; 0 and batch_tokens.shape[0] &lt; batch_size:\n                # Space left in the current batch\n                space_left = context_size - current_length\n\n                # If the current tokens fit entirely into the remaining space\n                if token_len &lt;= space_left:\n                    current_batch.append(tokens[:token_len])\n                    current_length += token_len\n                    break\n\n                else:\n                    # Take as much as will fit\n                    current_batch.append(tokens[:space_left])\n\n                    # Remove used part, add BOS\n                    tokens = tokens[space_left:]\n                    token_len -= space_left\n\n                    # only add BOS if it's not already the first token\n                    if self.prepend_bos:\n                        bos_token_id_tensor = torch.tensor(\n                            [self.model.tokenizer.bos_token_id],\n                            device=tokens.device,\n                            dtype=torch.long,\n                        )\n                        if tokens[0] != bos_token_id_tensor:\n                            tokens = torch.cat(\n                                (\n                                    bos_token_id_tensor,\n                                    tokens,\n                                ),\n                                dim=0,\n                            )\n                            token_len += 1\n                    current_length = context_size\n\n                # If a batch is full, concatenate and move to next batch\n                if current_length == context_size:\n                    full_batch = torch.cat(current_batch, dim=0)\n                    batch_tokens = torch.cat(\n                        (batch_tokens, full_batch.unsqueeze(0)), dim=0\n                    )\n                    current_batch = []\n                    current_length = 0\n\n            # pbar.n = batch_tokens.shape[0]\n            # pbar.refresh()\n        return batch_tokens[:batch_size].to(self.model.W_E.device)\n\n    @torch.no_grad()\n    def get_activations(self, batch_tokens: torch.Tensor):\n        \"\"\"\n        Returns activations of shape (batches, context, num_layers, d_in)\n\n        d_in may result from a concatenated head dimension.\n        \"\"\"\n\n        # Setup autocast if using\n        if self.autocast_lm:\n            autocast_if_enabled = torch.autocast(\n                device_type=\"cuda\",\n                dtype=torch.bfloat16,\n                enabled=self.autocast_lm,\n            )\n        else:\n            autocast_if_enabled = contextlib.nullcontext()\n\n        with autocast_if_enabled:\n            layerwise_activations = self.model.run_with_cache(\n                batch_tokens,\n                names_filter=[self.hook_name],\n                stop_at_layer=self.hook_layer + 1,\n                prepend_bos=self.prepend_bos,\n                **self.model_kwargs,\n            )[1]\n\n        n_batches, n_context = batch_tokens.shape\n\n        stacked_activations = torch.zeros((n_batches, n_context, 1, self.d_in))\n\n        if self.hook_head_index is not None:\n            stacked_activations[:, :, 0] = layerwise_activations[self.hook_name][\n                :, :, self.hook_head_index\n            ]\n        elif (\n            layerwise_activations[self.hook_name].ndim &gt; 3\n        ):  # if we have a head dimension\n            stacked_activations[:, :, 0] = layerwise_activations[self.hook_name].view(\n                n_batches, n_context, -1\n            )\n        else:\n            stacked_activations[:, :, 0] = layerwise_activations[self.hook_name]\n\n        return stacked_activations\n\n    @torch.no_grad()\n    def get_buffer(self, n_batches_in_buffer: int) -&gt; torch.Tensor:\n        context_size = self.context_size\n        batch_size = self.store_batch_size_prompts\n        d_in = self.d_in\n        total_size = batch_size * n_batches_in_buffer\n        num_layers = 1\n\n        if self.cached_activations_path is not None:\n            # Load the activations from disk\n            buffer_size = total_size * context_size\n            # Initialize an empty tensor with an additional dimension for layers\n            new_buffer = torch.zeros(\n                (buffer_size, num_layers, d_in),\n                dtype=self.dtype,  # type: ignore\n                device=self.device,\n            )\n            n_tokens_filled = 0\n\n            # Assume activations for different layers are stored separately and need to be combined\n            while n_tokens_filled &lt; buffer_size:\n                if not os.path.exists(\n                    f\"{self.cached_activations_path}/{self.next_cache_idx}.safetensors\"\n                ):\n                    print(\n                        \"\\n\\nWarning: Ran out of cached activation files earlier than expected.\"\n                    )\n                    print(\n                        f\"Expected to have {buffer_size} activations, but only found {n_tokens_filled}.\"\n                    )\n                    if buffer_size % self.total_training_tokens != 0:\n                        print(\n                            \"This might just be a rounding error \u2014 your batch_size * n_batches_in_buffer * context_size is not divisible by your total_training_tokens\"\n                        )\n                    print(f\"Returning a buffer of size {n_tokens_filled} instead.\")\n                    print(\"\\n\\n\")\n                    new_buffer = new_buffer[:n_tokens_filled, ...]\n                    return new_buffer\n\n                activations = self.load_buffer(\n                    f\"{self.cached_activations_path}/{self.next_cache_idx}.safetensors\"\n                )\n                taking_subset_of_file = False\n                if n_tokens_filled + activations.shape[0] &gt; buffer_size:\n                    activations = activations[: buffer_size - n_tokens_filled, ...]\n                    taking_subset_of_file = True\n\n                new_buffer[\n                    n_tokens_filled : n_tokens_filled + activations.shape[0], ...\n                ] = activations\n\n                if taking_subset_of_file:\n                    self.next_idx_within_buffer = activations.shape[0]\n                else:\n                    self.next_cache_idx += 1\n                    self.next_idx_within_buffer = 0\n\n                n_tokens_filled += activations.shape[0]\n\n            return new_buffer\n\n        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)\n        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n        new_buffer = torch.zeros(\n            (total_size, context_size, num_layers, d_in),\n            dtype=self.dtype,  # type: ignore\n            device=self.device,\n        )\n\n        for refill_batch_idx_start in refill_iterator:\n            # move batch toks to gpu for model\n            refill_batch_tokens = self.get_batch_tokens().to(self.model.cfg.device)\n            refill_activations = self.get_activations(refill_batch_tokens)\n            # move acts back to cpu\n            refill_activations.to(self.device)\n            new_buffer[\n                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n            ] = refill_activations\n\n            # pbar.update(1)\n\n        new_buffer = new_buffer.reshape(-1, num_layers, d_in)\n        new_buffer = new_buffer[torch.randperm(new_buffer.shape[0])]\n\n        # every buffer should be normalized:\n        if self.normalize_activations == \"expected_average_only_in\":\n            new_buffer = self.apply_norm_scaling_factor(new_buffer)\n\n        return new_buffer\n\n    def save_buffer(self, buffer: torch.Tensor, path: str):\n        \"\"\"\n        Used by cached activations runner to save a buffer to disk.\n        For reuse by later workflows.\n        \"\"\"\n        save_file({\"activations\": buffer}, path)\n\n    def load_buffer(self, path: str) -&gt; torch.Tensor:\n\n        with safe_open(path, framework=\"pt\", device=str(self.device)) as f:  # type: ignore\n            buffer = f.get_tensor(\"activations\")\n        return buffer\n\n    def get_data_loader(\n        self,\n    ) -&gt; Iterator[Any]:\n        \"\"\"\n        Return a torch.utils.dataloader which you can get batches from.\n\n        Should automatically refill the buffer when it gets to n % full.\n        (better mixing if you refill and shuffle regularly).\n\n        \"\"\"\n\n        batch_size = self.train_batch_size_tokens\n\n        # 1. # create new buffer by mixing stored and new buffer\n        mixing_buffer = torch.cat(\n            [self.get_buffer(self.n_batches_in_buffer // 2), self.storage_buffer],\n            dim=0,\n        )\n\n        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n\n        # 2.  put 50 % in storage\n        self._storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n\n        # 3. put other 50 % in a dataloader\n        dataloader = iter(\n            DataLoader(\n                # TODO: seems like a typing bug?\n                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n                batch_size=batch_size,\n                shuffle=True,\n            )\n        )\n\n        return dataloader\n\n    def next_batch(self):\n        \"\"\"\n        Get the next batch from the current DataLoader.\n        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n        \"\"\"\n        try:\n            # Try to get the next batch\n            return next(self.dataloader)\n        except StopIteration:\n            # If the DataLoader is exhausted, create a new one\n            self._dataloader = self.get_data_loader()\n            return next(self.dataloader)\n\n    def state_dict(self) -&gt; dict[str, torch.Tensor]:\n        result = {\n            \"n_dataset_processed\": torch.tensor(self.n_dataset_processed),\n        }\n        if self._storage_buffer is not None:  # first time might be None\n            result[\"storage_buffer\"] = self._storage_buffer\n        return result\n\n    def save(self, file_path: str):\n        save_file(self.state_dict(), file_path)\n\n    def _get_next_dataset_tokens(self) -&gt; torch.Tensor:\n        device = self.device\n        if not self.is_dataset_tokenized:\n            s = next(self.iterable_dataset)[self.tokens_column]\n            tokens = (\n                self.model.to_tokens(\n                    s,\n                    truncate=False,\n                    move_to_device=True,\n                    prepend_bos=self.prepend_bos,\n                )\n                .squeeze(0)\n                .to(device)\n            )\n            assert (\n                len(tokens.shape) == 1\n            ), f\"tokens.shape should be 1D but was {tokens.shape}\"\n        else:\n            tokens = torch.tensor(\n                next(self.iterable_dataset)[self.tokens_column],\n                dtype=torch.long,\n                device=device,\n                requires_grad=False,\n            )\n            if (\n                not self.prepend_bos\n                and tokens[0] == self.model.tokenizer.bos_token_id  # type: ignore\n            ):\n                tokens = tokens[1:]\n        self.n_dataset_processed += 1\n        return tokens\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.get_activations","title":"<code>get_activations(batch_tokens)</code>","text":"<p>Returns activations of shape (batches, context, num_layers, d_in)</p> <p>d_in may result from a concatenated head dimension.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>@torch.no_grad()\ndef get_activations(self, batch_tokens: torch.Tensor):\n    \"\"\"\n    Returns activations of shape (batches, context, num_layers, d_in)\n\n    d_in may result from a concatenated head dimension.\n    \"\"\"\n\n    # Setup autocast if using\n    if self.autocast_lm:\n        autocast_if_enabled = torch.autocast(\n            device_type=\"cuda\",\n            dtype=torch.bfloat16,\n            enabled=self.autocast_lm,\n        )\n    else:\n        autocast_if_enabled = contextlib.nullcontext()\n\n    with autocast_if_enabled:\n        layerwise_activations = self.model.run_with_cache(\n            batch_tokens,\n            names_filter=[self.hook_name],\n            stop_at_layer=self.hook_layer + 1,\n            prepend_bos=self.prepend_bos,\n            **self.model_kwargs,\n        )[1]\n\n    n_batches, n_context = batch_tokens.shape\n\n    stacked_activations = torch.zeros((n_batches, n_context, 1, self.d_in))\n\n    if self.hook_head_index is not None:\n        stacked_activations[:, :, 0] = layerwise_activations[self.hook_name][\n            :, :, self.hook_head_index\n        ]\n    elif (\n        layerwise_activations[self.hook_name].ndim &gt; 3\n    ):  # if we have a head dimension\n        stacked_activations[:, :, 0] = layerwise_activations[self.hook_name].view(\n            n_batches, n_context, -1\n        )\n    else:\n        stacked_activations[:, :, 0] = layerwise_activations[self.hook_name]\n\n    return stacked_activations\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.get_batch_tokens","title":"<code>get_batch_tokens(batch_size=None)</code>","text":"<p>Streams a batch of tokens from a dataset.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_batch_tokens(self, batch_size: int | None = None):\n    \"\"\"\n    Streams a batch of tokens from a dataset.\n    \"\"\"\n    if not batch_size:\n        batch_size = self.store_batch_size_prompts\n    context_size = self.context_size\n    device = self.device\n\n    batch_tokens = torch.zeros(\n        size=(0, context_size), device=device, dtype=torch.long, requires_grad=False\n    )\n\n    current_batch = []\n    current_length = 0\n\n    while batch_tokens.shape[0] &lt; batch_size:\n        tokens = self._get_next_dataset_tokens()\n        token_len = tokens.shape[0]\n\n        # TODO: Fix this so that we are limiting how many tokens we get from the same context.\n        assert self.model.tokenizer is not None  # keep pyright happy\n        while token_len &gt; 0 and batch_tokens.shape[0] &lt; batch_size:\n            # Space left in the current batch\n            space_left = context_size - current_length\n\n            # If the current tokens fit entirely into the remaining space\n            if token_len &lt;= space_left:\n                current_batch.append(tokens[:token_len])\n                current_length += token_len\n                break\n\n            else:\n                # Take as much as will fit\n                current_batch.append(tokens[:space_left])\n\n                # Remove used part, add BOS\n                tokens = tokens[space_left:]\n                token_len -= space_left\n\n                # only add BOS if it's not already the first token\n                if self.prepend_bos:\n                    bos_token_id_tensor = torch.tensor(\n                        [self.model.tokenizer.bos_token_id],\n                        device=tokens.device,\n                        dtype=torch.long,\n                    )\n                    if tokens[0] != bos_token_id_tensor:\n                        tokens = torch.cat(\n                            (\n                                bos_token_id_tensor,\n                                tokens,\n                            ),\n                            dim=0,\n                        )\n                        token_len += 1\n                current_length = context_size\n\n            # If a batch is full, concatenate and move to next batch\n            if current_length == context_size:\n                full_batch = torch.cat(current_batch, dim=0)\n                batch_tokens = torch.cat(\n                    (batch_tokens, full_batch.unsqueeze(0)), dim=0\n                )\n                current_batch = []\n                current_length = 0\n\n        # pbar.n = batch_tokens.shape[0]\n        # pbar.refresh()\n    return batch_tokens[:batch_size].to(self.model.W_E.device)\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.get_data_loader","title":"<code>get_data_loader()</code>","text":"<p>Return a torch.utils.dataloader which you can get batches from.</p> <p>Should automatically refill the buffer when it gets to n % full. (better mixing if you refill and shuffle regularly).</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_data_loader(\n    self,\n) -&gt; Iterator[Any]:\n    \"\"\"\n    Return a torch.utils.dataloader which you can get batches from.\n\n    Should automatically refill the buffer when it gets to n % full.\n    (better mixing if you refill and shuffle regularly).\n\n    \"\"\"\n\n    batch_size = self.train_batch_size_tokens\n\n    # 1. # create new buffer by mixing stored and new buffer\n    mixing_buffer = torch.cat(\n        [self.get_buffer(self.n_batches_in_buffer // 2), self.storage_buffer],\n        dim=0,\n    )\n\n    mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n\n    # 2.  put 50 % in storage\n    self._storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n\n    # 3. put other 50 % in a dataloader\n    dataloader = iter(\n        DataLoader(\n            # TODO: seems like a typing bug?\n            cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n            batch_size=batch_size,\n            shuffle=True,\n        )\n    )\n\n    return dataloader\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.next_batch","title":"<code>next_batch()</code>","text":"<p>Get the next batch from the current DataLoader. If the DataLoader is exhausted, refill the buffer and create a new DataLoader.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def next_batch(self):\n    \"\"\"\n    Get the next batch from the current DataLoader.\n    If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n    \"\"\"\n    try:\n        # Try to get the next batch\n        return next(self.dataloader)\n    except StopIteration:\n        # If the DataLoader is exhausted, create a new one\n        self._dataloader = self.get_data_loader()\n        return next(self.dataloader)\n</code></pre>"},{"location":"api/#sae_lens.ActivationsStore.save_buffer","title":"<code>save_buffer(buffer, path)</code>","text":"<p>Used by cached activations runner to save a buffer to disk. For reuse by later workflows.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def save_buffer(self, buffer: torch.Tensor, path: str):\n    \"\"\"\n    Used by cached activations runner to save a buffer to disk.\n    For reuse by later workflows.\n    \"\"\"\n    save_file({\"activations\": buffer}, path)\n</code></pre>"},{"location":"api/#sae_lens.CacheActivationsRunner","title":"<code>CacheActivationsRunner</code>","text":"Source code in <code>sae_lens/cache_activations_runner.py</code> <pre><code>class CacheActivationsRunner:\n\n    def __init__(self, cfg: CacheActivationsRunnerConfig):\n        self.cfg = cfg\n        self.model = load_model(\n            model_class_name=cfg.model_class_name,\n            model_name=cfg.model_name,\n            device=cfg.device,\n            model_from_pretrained_kwargs=cfg.model_from_pretrained_kwargs,\n        )\n        self.activations_store = ActivationsStore.from_config(\n            self.model,\n            cfg,\n        )\n\n        self.file_extension = \"safetensors\"\n\n    def __str__(self):\n        \"\"\"\n        Print the number of tokens to be cached.\n        Print the number of buffers, and the number of tokens per buffer.\n        Print the disk space required to store the activations.\n\n        \"\"\"\n\n        bytes_per_token = (\n            self.cfg.d_in * self.cfg.dtype.itemsize\n            if isinstance(self.cfg.dtype, torch.dtype)\n            else DTYPE_MAP[self.cfg.dtype].itemsize\n        )\n        tokens_in_buffer = (\n            self.cfg.n_batches_in_buffer\n            * self.cfg.store_batch_size_prompts\n            * self.cfg.context_size\n        )\n        total_training_tokens = self.cfg.training_tokens\n        total_disk_space_gb = total_training_tokens * bytes_per_token / 10**9\n\n        return (\n            f\"Activation Cache Runner:\\n\"\n            f\"Total training tokens: {total_training_tokens}\\n\"\n            f\"Number of buffers: {math.ceil(total_training_tokens / tokens_in_buffer)}\\n\"\n            f\"Tokens per buffer: {tokens_in_buffer}\\n\"\n            f\"Disk space required: {total_disk_space_gb:.2f} GB\\n\"\n            f\"Configuration:\\n\"\n            f\"{self.cfg}\"\n        )\n\n    @torch.no_grad()\n    def run(self):\n\n        new_cached_activations_path = self.cfg.new_cached_activations_path\n\n        # if the activations directory exists and has files in it, raise an exception\n        assert new_cached_activations_path is not None\n        if os.path.exists(new_cached_activations_path):\n            if len(os.listdir(new_cached_activations_path)) &gt; 0:\n                raise Exception(\n                    f\"Activations directory ({new_cached_activations_path}) is not empty. Please delete it or specify a different path. Exiting the script to prevent accidental deletion of files.\"\n                )\n        else:\n            os.makedirs(new_cached_activations_path)\n\n        print(f\"Started caching {self.cfg.training_tokens} activations\")\n        tokens_per_buffer = (\n            self.cfg.store_batch_size_prompts\n            * self.cfg.context_size\n            * self.cfg.n_batches_in_buffer\n        )\n\n        n_buffers = math.ceil(self.cfg.training_tokens / tokens_per_buffer)\n\n        for i in tqdm(range(n_buffers), desc=\"Caching activations\"):\n            buffer = self.activations_store.get_buffer(self.cfg.n_batches_in_buffer)\n            self.activations_store.save_buffer(\n                buffer, f\"{new_cached_activations_path}/{i}.safetensors\"\n            )\n\n            del buffer\n\n            if i % self.cfg.shuffle_every_n_buffers == 0 and i &gt; 0:\n                # Shuffle the buffers on disk\n\n                # Do random pairwise shuffling between the last shuffle_every_n_buffers buffers\n                for _ in range(self.cfg.n_shuffles_with_last_section):\n                    self.shuffle_activations_pairwise(\n                        new_cached_activations_path,\n                        buffer_idx_range=(i - self.cfg.shuffle_every_n_buffers, i),\n                    )\n\n                # Do more random pairwise shuffling between all the buffers\n                for _ in range(self.cfg.n_shuffles_in_entire_dir):\n                    self.shuffle_activations_pairwise(\n                        new_cached_activations_path,\n                        buffer_idx_range=(0, i),\n                    )\n\n        # More final shuffling (mostly in case we didn't end on an i divisible by shuffle_every_n_buffers)\n        if n_buffers &gt; 1:\n            for _ in tqdm(range(self.cfg.n_shuffles_final), desc=\"Final shuffling\"):\n                self.shuffle_activations_pairwise(\n                    new_cached_activations_path,\n                    buffer_idx_range=(0, n_buffers),\n                )\n\n    @torch.no_grad()\n    def shuffle_activations_pairwise(\n        self, datapath: str, buffer_idx_range: Tuple[int, int]\n    ):\n        \"\"\"\n        Shuffles two buffers on disk.\n        \"\"\"\n        assert (\n            buffer_idx_range[0] &lt; buffer_idx_range[1] - 1\n        ), \"buffer_idx_range[0] must be smaller than buffer_idx_range[1] by at least 1\"\n\n        buffer_idx1 = torch.randint(\n            buffer_idx_range[0], buffer_idx_range[1], (1,)\n        ).item()\n        buffer_idx2 = torch.randint(\n            buffer_idx_range[0], buffer_idx_range[1], (1,)\n        ).item()\n        while buffer_idx1 == buffer_idx2:  # Make sure they're not the same\n            buffer_idx2 = torch.randint(\n                buffer_idx_range[0], buffer_idx_range[1], (1,)\n            ).item()\n\n        buffer1 = self.activations_store.load_buffer(\n            f\"{datapath}/{buffer_idx1}.{self.file_extension}\"\n        )\n        buffer2 = self.activations_store.load_buffer(\n            f\"{datapath}/{buffer_idx2}.{self.file_extension}\"\n        )\n        joint_buffer = torch.cat([buffer1, buffer2])\n\n        # Shuffle them\n        joint_buffer = joint_buffer[torch.randperm(joint_buffer.shape[0])]\n        shuffled_buffer1 = joint_buffer[: buffer1.shape[0]]\n        shuffled_buffer2 = joint_buffer[buffer1.shape[0] :]\n\n        # Save them back\n        self.activations_store.save_buffer(\n            shuffled_buffer1, f\"{datapath}/{buffer_idx1}.{self.file_extension}\"\n        )\n        self.activations_store.save_buffer(\n            shuffled_buffer2, f\"{datapath}/{buffer_idx2}.{self.file_extension}\"\n        )\n</code></pre>"},{"location":"api/#sae_lens.CacheActivationsRunner.__str__","title":"<code>__str__()</code>","text":"<p>Print the number of tokens to be cached. Print the number of buffers, and the number of tokens per buffer. Print the disk space required to store the activations.</p> Source code in <code>sae_lens/cache_activations_runner.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Print the number of tokens to be cached.\n    Print the number of buffers, and the number of tokens per buffer.\n    Print the disk space required to store the activations.\n\n    \"\"\"\n\n    bytes_per_token = (\n        self.cfg.d_in * self.cfg.dtype.itemsize\n        if isinstance(self.cfg.dtype, torch.dtype)\n        else DTYPE_MAP[self.cfg.dtype].itemsize\n    )\n    tokens_in_buffer = (\n        self.cfg.n_batches_in_buffer\n        * self.cfg.store_batch_size_prompts\n        * self.cfg.context_size\n    )\n    total_training_tokens = self.cfg.training_tokens\n    total_disk_space_gb = total_training_tokens * bytes_per_token / 10**9\n\n    return (\n        f\"Activation Cache Runner:\\n\"\n        f\"Total training tokens: {total_training_tokens}\\n\"\n        f\"Number of buffers: {math.ceil(total_training_tokens / tokens_in_buffer)}\\n\"\n        f\"Tokens per buffer: {tokens_in_buffer}\\n\"\n        f\"Disk space required: {total_disk_space_gb:.2f} GB\\n\"\n        f\"Configuration:\\n\"\n        f\"{self.cfg}\"\n    )\n</code></pre>"},{"location":"api/#sae_lens.CacheActivationsRunner.shuffle_activations_pairwise","title":"<code>shuffle_activations_pairwise(datapath, buffer_idx_range)</code>","text":"<p>Shuffles two buffers on disk.</p> Source code in <code>sae_lens/cache_activations_runner.py</code> <pre><code>@torch.no_grad()\ndef shuffle_activations_pairwise(\n    self, datapath: str, buffer_idx_range: Tuple[int, int]\n):\n    \"\"\"\n    Shuffles two buffers on disk.\n    \"\"\"\n    assert (\n        buffer_idx_range[0] &lt; buffer_idx_range[1] - 1\n    ), \"buffer_idx_range[0] must be smaller than buffer_idx_range[1] by at least 1\"\n\n    buffer_idx1 = torch.randint(\n        buffer_idx_range[0], buffer_idx_range[1], (1,)\n    ).item()\n    buffer_idx2 = torch.randint(\n        buffer_idx_range[0], buffer_idx_range[1], (1,)\n    ).item()\n    while buffer_idx1 == buffer_idx2:  # Make sure they're not the same\n        buffer_idx2 = torch.randint(\n            buffer_idx_range[0], buffer_idx_range[1], (1,)\n        ).item()\n\n    buffer1 = self.activations_store.load_buffer(\n        f\"{datapath}/{buffer_idx1}.{self.file_extension}\"\n    )\n    buffer2 = self.activations_store.load_buffer(\n        f\"{datapath}/{buffer_idx2}.{self.file_extension}\"\n    )\n    joint_buffer = torch.cat([buffer1, buffer2])\n\n    # Shuffle them\n    joint_buffer = joint_buffer[torch.randperm(joint_buffer.shape[0])]\n    shuffled_buffer1 = joint_buffer[: buffer1.shape[0]]\n    shuffled_buffer2 = joint_buffer[buffer1.shape[0] :]\n\n    # Save them back\n    self.activations_store.save_buffer(\n        shuffled_buffer1, f\"{datapath}/{buffer_idx1}.{self.file_extension}\"\n    )\n    self.activations_store.save_buffer(\n        shuffled_buffer2, f\"{datapath}/{buffer_idx2}.{self.file_extension}\"\n    )\n</code></pre>"},{"location":"api/#sae_lens.CacheActivationsRunnerConfig","title":"<code>CacheActivationsRunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for caching activations of an LLM.</p> Source code in <code>sae_lens/config.py</code> <pre><code>@dataclass\nclass CacheActivationsRunnerConfig:\n    \"\"\"\n    Configuration for caching activations of an LLM.\n    \"\"\"\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name: str = \"gelu-2l\"\n    model_class_name: str = \"HookedTransformer\"\n    hook_name: str = \"blocks.{layer}.hook_mlp_out\"\n    hook_layer: int = 0\n    hook_head_index: Optional[int] = None\n    dataset_path: str = \"NeelNanda/c4-tokenized-2b\"\n    dataset_trust_remote_code: bool | None = None\n    streaming: bool = True\n    is_dataset_tokenized: bool = True\n    context_size: int = 128\n    new_cached_activations_path: Optional[str] = (\n        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}\"\n    )\n    # dont' specify this since you don't want to load from disk with the cache runner.\n    cached_activations_path: Optional[str] = None\n    # SAE Parameters\n    d_in: int = 512\n\n    # Activation Store Parameters\n    n_batches_in_buffer: int = 20\n    training_tokens: int = 2_000_000\n    store_batch_size_prompts: int = 32\n    train_batch_size_tokens: int = 4096\n    normalize_activations: str = \"none\"  # should always be none for activation caching\n\n    # Misc\n    device: str = \"cpu\"\n    act_store_device: str = \"with_model\"  # will be set by post init if with_model\n    seed: int = 42\n    dtype: str = \"float32\"\n    prepend_bos: bool = True\n    autocast_lm: bool = False  # autocast lm during activation fetching\n\n    # Activation caching stuff\n    shuffle_every_n_buffers: int = 10\n    n_shuffles_with_last_section: int = 10\n    n_shuffles_in_entire_dir: int = 10\n    n_shuffles_final: int = 100\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    model_from_pretrained_kwargs: dict[str, Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        # Autofill cached_activations_path unless the user overrode it\n        if self.new_cached_activations_path is None:\n            self.new_cached_activations_path = _default_cached_activations_path(\n                self.dataset_path,\n                self.model_name,\n                self.hook_name,\n                self.hook_head_index,\n            )\n\n        if self.act_store_device == \"with_model\":\n            self.act_store_device = self.device\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer","title":"<code>HookedSAETransformer</code>","text":"<p>             Bases: <code>HookedTransformer</code></p> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>class HookedSAETransformer(HookedTransformer):\n\n    def __init__(\n        self,\n        *model_args: Any,\n        **model_kwargs: Any,\n    ):\n        \"\"\"Model initialization. Just HookedTransformer init, but adds a dictionary to keep track of attached SAEs.\n\n        Note that if you want to load the model from pretrained weights, you should use\n        :meth:`from_pretrained` instead.\n\n        Args:\n            *model_args: Positional arguments for HookedTransformer initialization\n            **model_kwargs: Keyword arguments for HookedTransformer initialization\n        \"\"\"\n        super().__init__(*model_args, **model_kwargs)\n        self.acts_to_saes: Dict[str, SAE] = {}\n\n    def add_sae(self, sae: SAE):\n        \"\"\"Attaches an SAE to the model\n\n        WARNING: This sae will be permanantly attached until you remove it with reset_saes. This function will also overwrite any existing SAE attached to the same hook point.\n\n        Args:\n            sae: SparseAutoencoderBase. The SAE to attach to the model\n        \"\"\"\n        act_name = sae.cfg.hook_name\n        if (act_name not in self.acts_to_saes) and (act_name not in self.hook_dict):\n            logging.warning(\n                f\"No hook found for {act_name}. Skipping. Check model.hook_dict for available hooks.\"\n            )\n            return\n\n        self.acts_to_saes[act_name] = sae\n        set_deep_attr(self, act_name, sae)\n        self.setup()\n\n    def _reset_sae(self, act_name: str, prev_sae: Optional[SAE] = None):\n        \"\"\"Resets an SAE that was attached to the model\n\n        By default will remove the SAE from that hook_point.\n        If prev_sae is provided, will replace the current SAE with the provided one.\n        This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes)\n\n        Args:\n            act_name: str. The hook_name of the SAE to reset\n            prev_sae: Optional[HookedSAE]. The SAE to replace the current one with. If None, will just remove the SAE from this hook point. Defaults to None\n        \"\"\"\n        if act_name not in self.acts_to_saes:\n            logging.warning(\n                f\"No SAE is attached to {act_name}. There's nothing to reset.\"\n            )\n            return\n\n        if prev_sae:\n            set_deep_attr(self, act_name, prev_sae)\n            self.acts_to_saes[act_name] = prev_sae\n        else:\n            set_deep_attr(self, act_name, HookPoint())\n            del self.acts_to_saes[act_name]\n\n    def reset_saes(\n        self,\n        act_names: Optional[Union[str, List[str]]] = None,\n        prev_saes: Optional[List[Union[SAE, None]]] = None,\n    ):\n        \"\"\"Reset the SAEs attached to the model\n\n        If act_names are provided will just reset SAEs attached to those hooks. Otherwise will reset all SAEs attached to the model.\n        Optionally can provide a list of prev_saes to reset to. This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes).\n\n        Args:\n            act_names (Optional[Union[str, List[str]]): The act_names of the SAEs to reset. If None, will reset all SAEs attached to the model. Defaults to None.\n            prev_saes (Optional[List[Union[HookedSAE, None]]]): List of SAEs to replace the current ones with. If None, will just remove the SAEs. Defaults to None.\n        \"\"\"\n        if isinstance(act_names, str):\n            act_names = [act_names]\n        elif act_names is None:\n            act_names = list(self.acts_to_saes.keys())\n\n        if prev_saes:\n            assert len(act_names) == len(\n                prev_saes\n            ), \"act_names and prev_saes must have the same length\"\n        else:\n            prev_saes = [None] * len(act_names)  # type: ignore\n\n        for act_name, prev_sae in zip(act_names, prev_saes):  # type: ignore\n            self._reset_sae(act_name, prev_sae)\n\n        self.setup()\n\n    def run_with_saes(\n        self,\n        *model_args: Any,\n        saes: Union[SAE, List[SAE]] = [],\n        reset_saes_end: bool = True,\n        **model_kwargs: Any,\n    ) -&gt; Union[\n        None,\n        Float[torch.Tensor, \"batch pos d_vocab\"],\n        Loss,\n        Tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n    ]:\n        \"\"\"Wrapper around HookedTransformer forward pass.\n\n        Runs the model with the given SAEs attached for one forward pass, then removes them. By default, will reset all SAEs to original state after.\n\n        Args:\n            *model_args: Positional arguments for the model forward pass\n            saes: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass\n            reset_saes_end (bool): If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n            **model_kwargs: Keyword arguments for the model forward pass\n        \"\"\"\n        with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n            return self(*model_args, **model_kwargs)\n\n    def run_with_cache_with_saes(\n        self,\n        *model_args: Any,\n        saes: Union[SAE, List[SAE]] = [],\n        reset_saes_end: bool = True,\n        return_cache_object: bool = True,\n        remove_batch_dim: bool = False,\n        **kwargs: Any,\n    ) -&gt; Tuple[\n        Union[\n            None,\n            Float[torch.Tensor, \"batch pos d_vocab\"],\n            Loss,\n            Tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n        ],\n        Union[ActivationCache, Dict[str, torch.Tensor]],\n    ]:\n        \"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\n\n        Attaches given SAEs before running the model with cache and then removes them.\n        By default, will reset all SAEs to original state after.\n\n        Args:\n            *model_args: Positional arguments for the model forward pass\n            saes: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass\n            reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n            return_cache_object: (bool) if True, this will return an ActivationCache object, with a bunch of\n                useful HookedTransformer specific methods, otherwise it will return a dictionary of\n                activations as in HookedRootModule.\n            remove_batch_dim: (bool) Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.\n            **kwargs: Keyword arguments for the model forward pass\n        \"\"\"\n        with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n            return self.run_with_cache(  # type: ignore\n                *model_args,\n                return_cache_object=return_cache_object,  # type: ignore\n                remove_batch_dim=remove_batch_dim,\n                **kwargs,\n            )\n\n    def run_with_hooks_with_saes(\n        self,\n        *model_args: Any,\n        saes: Union[SAE, List[SAE]] = [],\n        reset_saes_end: bool = True,\n        fwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],  # type: ignore\n        bwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],  # type: ignore\n        reset_hooks_end: bool = True,\n        clear_contexts: bool = False,\n        **model_kwargs: Any,\n    ):\n        \"\"\"Wrapper around 'run_with_hooks' in HookedTransformer.\n\n        Attaches the given SAEs to the model before running the model with hooks and then removes them.\n        By default, will reset all SAEs to original state after.\n\n        Args:\n            *model_args: Positional arguments for the model forward pass\n            act_names: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass\n            reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. (default: True)\n            fwd_hooks: (List[Tuple[Union[str, Callable], Callable]]) List of forward hooks to apply\n            bwd_hooks: (List[Tuple[Union[str, Callable], Callable]]) List of backward hooks to apply\n            reset_hooks_end: (bool) Whether to reset the hooks at the end of the forward pass (default: True)\n            clear_contexts: (bool) Whether to clear the contexts at the end of the forward pass (default: False)\n            **model_kwargs: Keyword arguments for the model forward pass\n        \"\"\"\n        with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n            return self.run_with_hooks(\n                *model_args,\n                fwd_hooks=fwd_hooks,\n                bwd_hooks=bwd_hooks,\n                reset_hooks_end=reset_hooks_end,\n                clear_contexts=clear_contexts,\n                **model_kwargs,\n            )\n\n    @contextmanager\n    def saes(\n        self,\n        saes: Union[SAE, List[SAE]] = [],\n        reset_saes_end: bool = True,\n    ):\n        \"\"\"\n        A context manager for adding temporary SAEs to the model.\n        See HookedTransformer.hooks for a similar context manager for hooks.\n        By default will keep track of previously attached SAEs, and restore them when the context manager exits.\n\n        Example:\n\n        .. code-block:: python\n\n            from transformer_lens import HookedSAETransformer, HookedSAE, HookedSAEConfig\n\n            model = HookedSAETransformer.from_pretrained('gpt2-small')\n            sae_cfg = HookedSAEConfig(...)\n            sae = HookedSAE(sae_cfg)\n            with model.saes(saes=[sae]):\n                spliced_logits = model(text)\n\n\n        Args:\n            saes (Union[HookedSAE, List[HookedSAE]]): SAEs to be attached.\n            reset_saes_end (bool): If True, removes all SAEs added by this context manager when the context manager exits, returning previously attached SAEs to their original state.\n        \"\"\"\n        act_names_to_reset = []\n        prev_saes = []\n        if isinstance(saes, SAE):\n            saes = [saes]\n        try:\n            for sae in saes:\n                act_names_to_reset.append(sae.cfg.hook_name)\n                prev_saes.append(self.acts_to_saes.get(sae.cfg.hook_name, None))\n                self.add_sae(sae)\n            yield self\n        finally:\n            if reset_saes_end:\n                self.reset_saes(act_names_to_reset, prev_saes)\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.__init__","title":"<code>__init__(*model_args, **model_kwargs)</code>","text":"<p>Model initialization. Just HookedTransformer init, but adds a dictionary to keep track of attached SAEs.</p> <p>Note that if you want to load the model from pretrained weights, you should use :meth:<code>from_pretrained</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for HookedTransformer initialization</p> <code>()</code> <code>**model_kwargs</code> <code>Any</code> <p>Keyword arguments for HookedTransformer initialization</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def __init__(\n    self,\n    *model_args: Any,\n    **model_kwargs: Any,\n):\n    \"\"\"Model initialization. Just HookedTransformer init, but adds a dictionary to keep track of attached SAEs.\n\n    Note that if you want to load the model from pretrained weights, you should use\n    :meth:`from_pretrained` instead.\n\n    Args:\n        *model_args: Positional arguments for HookedTransformer initialization\n        **model_kwargs: Keyword arguments for HookedTransformer initialization\n    \"\"\"\n    super().__init__(*model_args, **model_kwargs)\n    self.acts_to_saes: Dict[str, SAE] = {}\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.add_sae","title":"<code>add_sae(sae)</code>","text":"<p>Attaches an SAE to the model</p> <p>WARNING: This sae will be permanantly attached until you remove it with reset_saes. This function will also overwrite any existing SAE attached to the same hook point.</p> <p>Parameters:</p> Name Type Description Default <code>sae</code> <code>SAE</code> <p>SparseAutoencoderBase. The SAE to attach to the model</p> required Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def add_sae(self, sae: SAE):\n    \"\"\"Attaches an SAE to the model\n\n    WARNING: This sae will be permanantly attached until you remove it with reset_saes. This function will also overwrite any existing SAE attached to the same hook point.\n\n    Args:\n        sae: SparseAutoencoderBase. The SAE to attach to the model\n    \"\"\"\n    act_name = sae.cfg.hook_name\n    if (act_name not in self.acts_to_saes) and (act_name not in self.hook_dict):\n        logging.warning(\n            f\"No hook found for {act_name}. Skipping. Check model.hook_dict for available hooks.\"\n        )\n        return\n\n    self.acts_to_saes[act_name] = sae\n    set_deep_attr(self, act_name, sae)\n    self.setup()\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.reset_saes","title":"<code>reset_saes(act_names=None, prev_saes=None)</code>","text":"<p>Reset the SAEs attached to the model</p> <p>If act_names are provided will just reset SAEs attached to those hooks. Otherwise will reset all SAEs attached to the model. Optionally can provide a list of prev_saes to reset to. This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes).</p> <p>Parameters:</p> Name Type Description Default <code>act_names</code> <code>Optional[Union[str, List[str]]</code> <p>The act_names of the SAEs to reset. If None, will reset all SAEs attached to the model. Defaults to None.</p> <code>None</code> <code>prev_saes</code> <code>Optional[List[Union[HookedSAE, None]]]</code> <p>List of SAEs to replace the current ones with. If None, will just remove the SAEs. Defaults to None.</p> <code>None</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def reset_saes(\n    self,\n    act_names: Optional[Union[str, List[str]]] = None,\n    prev_saes: Optional[List[Union[SAE, None]]] = None,\n):\n    \"\"\"Reset the SAEs attached to the model\n\n    If act_names are provided will just reset SAEs attached to those hooks. Otherwise will reset all SAEs attached to the model.\n    Optionally can provide a list of prev_saes to reset to. This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes).\n\n    Args:\n        act_names (Optional[Union[str, List[str]]): The act_names of the SAEs to reset. If None, will reset all SAEs attached to the model. Defaults to None.\n        prev_saes (Optional[List[Union[HookedSAE, None]]]): List of SAEs to replace the current ones with. If None, will just remove the SAEs. Defaults to None.\n    \"\"\"\n    if isinstance(act_names, str):\n        act_names = [act_names]\n    elif act_names is None:\n        act_names = list(self.acts_to_saes.keys())\n\n    if prev_saes:\n        assert len(act_names) == len(\n            prev_saes\n        ), \"act_names and prev_saes must have the same length\"\n    else:\n        prev_saes = [None] * len(act_names)  # type: ignore\n\n    for act_name, prev_sae in zip(act_names, prev_saes):  # type: ignore\n        self._reset_sae(act_name, prev_sae)\n\n    self.setup()\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.run_with_cache_with_saes","title":"<code>run_with_cache_with_saes(*model_args, saes=[], reset_saes_end=True, return_cache_object=True, remove_batch_dim=False, **kwargs)</code>","text":"<p>Wrapper around 'run_with_cache' in HookedTransformer.</p> <p>Attaches given SAEs before running the model with cache and then removes them. By default, will reset all SAEs to original state after.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for the model forward pass</p> <code>()</code> <code>saes</code> <code>Union[SAE, List[SAE]]</code> <p>(Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass</p> <code>[]</code> <code>reset_saes_end</code> <code>bool</code> <p>(bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.</p> <code>True</code> <code>return_cache_object</code> <code>bool</code> <p>(bool) if True, this will return an ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a dictionary of activations as in HookedRootModule.</p> <code>True</code> <code>remove_batch_dim</code> <code>bool</code> <p>(bool) Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for the model forward pass</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def run_with_cache_with_saes(\n    self,\n    *model_args: Any,\n    saes: Union[SAE, List[SAE]] = [],\n    reset_saes_end: bool = True,\n    return_cache_object: bool = True,\n    remove_batch_dim: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[\n    Union[\n        None,\n        Float[torch.Tensor, \"batch pos d_vocab\"],\n        Loss,\n        Tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n    ],\n    Union[ActivationCache, Dict[str, torch.Tensor]],\n]:\n    \"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\n\n    Attaches given SAEs before running the model with cache and then removes them.\n    By default, will reset all SAEs to original state after.\n\n    Args:\n        *model_args: Positional arguments for the model forward pass\n        saes: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass\n        reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n        return_cache_object: (bool) if True, this will return an ActivationCache object, with a bunch of\n            useful HookedTransformer specific methods, otherwise it will return a dictionary of\n            activations as in HookedRootModule.\n        remove_batch_dim: (bool) Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.\n        **kwargs: Keyword arguments for the model forward pass\n    \"\"\"\n    with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n        return self.run_with_cache(  # type: ignore\n            *model_args,\n            return_cache_object=return_cache_object,  # type: ignore\n            remove_batch_dim=remove_batch_dim,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.run_with_hooks_with_saes","title":"<code>run_with_hooks_with_saes(*model_args, saes=[], reset_saes_end=True, fwd_hooks=[], bwd_hooks=[], reset_hooks_end=True, clear_contexts=False, **model_kwargs)</code>","text":"<p>Wrapper around 'run_with_hooks' in HookedTransformer.</p> <p>Attaches the given SAEs to the model before running the model with hooks and then removes them. By default, will reset all SAEs to original state after.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for the model forward pass</p> <code>()</code> <code>act_names</code> <p>(Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass</p> required <code>reset_saes_end</code> <code>bool</code> <p>(bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. (default: True)</p> <code>True</code> <code>fwd_hooks</code> <code>List[Tuple[Union[str, Callable], Callable]]</code> <p>(List[Tuple[Union[str, Callable], Callable]]) List of forward hooks to apply</p> <code>[]</code> <code>bwd_hooks</code> <code>List[Tuple[Union[str, Callable], Callable]]</code> <p>(List[Tuple[Union[str, Callable], Callable]]) List of backward hooks to apply</p> <code>[]</code> <code>reset_hooks_end</code> <code>bool</code> <p>(bool) Whether to reset the hooks at the end of the forward pass (default: True)</p> <code>True</code> <code>clear_contexts</code> <code>bool</code> <p>(bool) Whether to clear the contexts at the end of the forward pass (default: False)</p> <code>False</code> <code>**model_kwargs</code> <code>Any</code> <p>Keyword arguments for the model forward pass</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def run_with_hooks_with_saes(\n    self,\n    *model_args: Any,\n    saes: Union[SAE, List[SAE]] = [],\n    reset_saes_end: bool = True,\n    fwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],  # type: ignore\n    bwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],  # type: ignore\n    reset_hooks_end: bool = True,\n    clear_contexts: bool = False,\n    **model_kwargs: Any,\n):\n    \"\"\"Wrapper around 'run_with_hooks' in HookedTransformer.\n\n    Attaches the given SAEs to the model before running the model with hooks and then removes them.\n    By default, will reset all SAEs to original state after.\n\n    Args:\n        *model_args: Positional arguments for the model forward pass\n        act_names: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass\n        reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. (default: True)\n        fwd_hooks: (List[Tuple[Union[str, Callable], Callable]]) List of forward hooks to apply\n        bwd_hooks: (List[Tuple[Union[str, Callable], Callable]]) List of backward hooks to apply\n        reset_hooks_end: (bool) Whether to reset the hooks at the end of the forward pass (default: True)\n        clear_contexts: (bool) Whether to clear the contexts at the end of the forward pass (default: False)\n        **model_kwargs: Keyword arguments for the model forward pass\n    \"\"\"\n    with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n        return self.run_with_hooks(\n            *model_args,\n            fwd_hooks=fwd_hooks,\n            bwd_hooks=bwd_hooks,\n            reset_hooks_end=reset_hooks_end,\n            clear_contexts=clear_contexts,\n            **model_kwargs,\n        )\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.run_with_saes","title":"<code>run_with_saes(*model_args, saes=[], reset_saes_end=True, **model_kwargs)</code>","text":"<p>Wrapper around HookedTransformer forward pass.</p> <p>Runs the model with the given SAEs attached for one forward pass, then removes them. By default, will reset all SAEs to original state after.</p> <p>Parameters:</p> Name Type Description Default <code>*model_args</code> <code>Any</code> <p>Positional arguments for the model forward pass</p> <code>()</code> <code>saes</code> <code>Union[SAE, List[SAE]]</code> <p>(Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass</p> <code>[]</code> <code>reset_saes_end</code> <code>bool</code> <p>If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.</p> <code>True</code> <code>**model_kwargs</code> <code>Any</code> <p>Keyword arguments for the model forward pass</p> <code>{}</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>def run_with_saes(\n    self,\n    *model_args: Any,\n    saes: Union[SAE, List[SAE]] = [],\n    reset_saes_end: bool = True,\n    **model_kwargs: Any,\n) -&gt; Union[\n    None,\n    Float[torch.Tensor, \"batch pos d_vocab\"],\n    Loss,\n    Tuple[Float[torch.Tensor, \"batch pos d_vocab\"], Loss],\n]:\n    \"\"\"Wrapper around HookedTransformer forward pass.\n\n    Runs the model with the given SAEs attached for one forward pass, then removes them. By default, will reset all SAEs to original state after.\n\n    Args:\n        *model_args: Positional arguments for the model forward pass\n        saes: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass\n        reset_saes_end (bool): If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.\n        **model_kwargs: Keyword arguments for the model forward pass\n    \"\"\"\n    with self.saes(saes=saes, reset_saes_end=reset_saes_end):\n        return self(*model_args, **model_kwargs)\n</code></pre>"},{"location":"api/#sae_lens.HookedSAETransformer.saes","title":"<code>saes(saes=[], reset_saes_end=True)</code>","text":"<p>A context manager for adding temporary SAEs to the model. See HookedTransformer.hooks for a similar context manager for hooks. By default will keep track of previously attached SAEs, and restore them when the context manager exits.</p> <p>Example:</p> <p>.. code-block:: python</p> <pre><code>from transformer_lens import HookedSAETransformer, HookedSAE, HookedSAEConfig\n\nmodel = HookedSAETransformer.from_pretrained('gpt2-small')\nsae_cfg = HookedSAEConfig(...)\nsae = HookedSAE(sae_cfg)\nwith model.saes(saes=[sae]):\n    spliced_logits = model(text)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>saes</code> <code>Union[HookedSAE, List[HookedSAE]]</code> <p>SAEs to be attached.</p> <code>[]</code> <code>reset_saes_end</code> <code>bool</code> <p>If True, removes all SAEs added by this context manager when the context manager exits, returning previously attached SAEs to their original state.</p> <code>True</code> Source code in <code>sae_lens/analysis/hooked_sae_transformer.py</code> <pre><code>@contextmanager\ndef saes(\n    self,\n    saes: Union[SAE, List[SAE]] = [],\n    reset_saes_end: bool = True,\n):\n    \"\"\"\n    A context manager for adding temporary SAEs to the model.\n    See HookedTransformer.hooks for a similar context manager for hooks.\n    By default will keep track of previously attached SAEs, and restore them when the context manager exits.\n\n    Example:\n\n    .. code-block:: python\n\n        from transformer_lens import HookedSAETransformer, HookedSAE, HookedSAEConfig\n\n        model = HookedSAETransformer.from_pretrained('gpt2-small')\n        sae_cfg = HookedSAEConfig(...)\n        sae = HookedSAE(sae_cfg)\n        with model.saes(saes=[sae]):\n            spliced_logits = model(text)\n\n\n    Args:\n        saes (Union[HookedSAE, List[HookedSAE]]): SAEs to be attached.\n        reset_saes_end (bool): If True, removes all SAEs added by this context manager when the context manager exits, returning previously attached SAEs to their original state.\n    \"\"\"\n    act_names_to_reset = []\n    prev_saes = []\n    if isinstance(saes, SAE):\n        saes = [saes]\n    try:\n        for sae in saes:\n            act_names_to_reset.append(sae.cfg.hook_name)\n            prev_saes.append(self.acts_to_saes.get(sae.cfg.hook_name, None))\n            self.add_sae(sae)\n        yield self\n    finally:\n        if reset_saes_end:\n            self.reset_saes(act_names_to_reset, prev_saes)\n</code></pre>"},{"location":"api/#sae_lens.LanguageModelSAERunnerConfig","title":"<code>LanguageModelSAERunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for training a sparse autoencoder on a language model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use. This should be the name of the model in the Hugging Face model hub.</p> <code>'gelu-2l'</code> <code>model_class_name</code> <code>str</code> <p>The name of the class of the model to use. This should be either <code>HookedTransformer</code> or <code>HookedMamba</code>.</p> <code>'HookedTransformer'</code> <code>hook_name</code> <code>str</code> <p>The name of the hook to use. This should be a valid TransformerLens hook.</p> <code>'blocks.0.hook_mlp_out'</code> <code>hook_eval</code> <code>str</code> <p>NOT CURRENTLY IN USE. The name of the hook to use for evaluation.</p> <code>'NOT_IN_USE'</code> <code>hook_layer</code> <code>int</code> <p>The index of the layer to hook. Used to stop forward passes early and speed up processing.</p> <code>0</code> <code>hook_head_index</code> <code>int</code> <p>When the hook if for an activatio with a head index, we can specify a specific head to use here.</p> <code>None</code> <code>dataset_path</code> <code>str</code> <p>A Hugging Face dataset path.</p> <code>'NeelNanda/c4-tokenized-2b'</code> <code>dataset_trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code when loading datasets from Huggingface.</p> <code>True</code> <code>streaming</code> <code>bool</code> <p>Whether to stream the dataset. Streaming large datasets is usually practical.</p> <code>True</code> <code>is_dataset_tokenized</code> <code>bool</code> <p>NOT IN USE. We used to use this but now automatically detect if the dataset is tokenized.</p> <code>True</code> <code>context_size</code> <code>int</code> <p>The context size to use when generating activations on which to train the SAE.</p> <code>128</code> <code>use_cached_activations</code> <code>bool</code> <p>Whether to use cached activations. This is useful when doing sweeps over the same activations.</p> <code>False</code> <code>cached_activations_path</code> <code>str</code> <p>The path to the cached activations.</p> <code>None</code> <code>d_in</code> <code>int</code> <p>The input dimension of the SAE.</p> <code>512</code> <code>d_sae</code> <code>int</code> <p>The output dimension of the SAE. If None, defaults to <code>d_in * expansion_factor</code>.</p> <code>None</code> <code>b_dec_init_method</code> <code>str</code> <p>The method to use to initialize the decoder bias. Zeros is likely fine.</p> <code>'geometric_median'</code> <code>expansion_factor</code> <code>int</code> <p>The expansion factor. Larger is better but more computationally expensive.</p> <code>4</code> <code>activation_fn</code> <code>str</code> <p>The activation function to use. Relu is standard.</p> <code>'relu'</code> <code>normalize_sae_decoder</code> <code>bool</code> <p>Whether to normalize the SAE decoder. Unit normed decoder weights used to be preferred.</p> <code>True</code> <code>noise_scale</code> <code>float</code> <p>Using noise to induce sparsity is supported but not recommended.</p> <code>0.0</code> <code>from_pretrained_path</code> <code>str</code> <p>The path to a pretrained SAE. We can finetune an existing SAE if needed.</p> <code>None</code> <code>apply_b_dec_to_input</code> <code>bool</code> <p>Whether to apply the decoder bias to the input. Not currently advised.</p> <code>True</code> <code>decoder_orthogonal_init</code> <code>bool</code> <p>Whether to use orthogonal initialization for the decoder. Not currently advised.</p> <code>False</code> <code>decoder_heuristic_init</code> <code>bool</code> <p>Whether to use heuristic initialization for the decoder. See Anthropic April Update.</p> <code>False</code> <code>init_encoder_as_decoder_transpose</code> <code>bool</code> <p>Whether to initialize the encoder as the transpose of the decoder. See Anthropic April Update.</p> <code>False</code> <code>n_batches_in_buffer</code> <code>int</code> <p>The number of batches in the buffer. When not using cached activations, a buffer in ram is used. The larger it is, the better shuffled the activations will be.</p> <code>20</code> <code>training_tokens</code> <code>int</code> <p>The number of training tokens.</p> <code>2000000</code> <code>finetuning_tokens</code> <code>int</code> <p>The number of finetuning tokens. See here</p> <code>0</code> <code>store_batch_size_prompts</code> <code>int</code> <p>The batch size for storing activations. This controls how many prompts are in the batch of the language model when generating actiations.</p> <code>32</code> <code>train_batch_size_tokens</code> <code>int</code> <p>The batch size for training. This controls the batch size of the SAE Training loop.</p> <code>4096</code> <code>normalize_activations</code> <code>str</code> <p>Activation Normalization Strategy. Either none, expected_average_only_in (estimate the average activation norm and divide activations by it -&gt; this can be folded post training and set to None), or constant_norm_rescale (at runtime set activation norm to sqrt(d_in) and then scale up the SAE output).</p> <code>'none'</code> <code>device</code> <code>str</code> <p>The device to use. Usually cuda.</p> <code>'cpu'</code> <code>act_store_device</code> <code>str</code> <p>The device to use for the activation store. CPU is advised in order to save vram.</p> <code>'with_model'</code> <code>seed</code> <code>int</code> <p>The seed to use.</p> <code>42</code> <code>dtype</code> <code>str</code> <p>The data type to use.</p> <code>'float32'</code> <code>prepend_bos</code> <code>bool</code> <p>Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.</p> <code>True</code> <code>autocast</code> <code>bool</code> <p>Whether to use autocast during training. Saves vram.</p> <code>False</code> <code>autocast_lm</code> <code>bool</code> <p>Whether to use autocast during activation fetching.</p> <code>False</code> <code>compile_llm</code> <code>bool</code> <p>Whether to compile the LLM.</p> <code>False</code> <code>llm_compilation_mode</code> <code>str</code> <p>The compilation mode to use for the LLM.</p> <code>None</code> <code>compile_sae</code> <code>bool</code> <p>Whether to compile the SAE.</p> <code>False</code> <code>sae_compilation_mode</code> <code>str</code> <p>The compilation mode to use for the SAE.</p> <code>None</code> <code>train_batch_size_tokens</code> <code>int</code> <p>The batch size for training.</p> <code>4096</code> <code>adam_beta1</code> <code>float</code> <p>The beta1 parameter for Adam.</p> <code>0</code> <code>adam_beta2</code> <code>float</code> <p>The beta2 parameter for Adam.</p> <code>0.999</code> <code>mse_loss_normalization</code> <code>str</code> <p>The normalization to use for the MSE loss.</p> <code>None</code> <code>l1_coefficient</code> <code>float</code> <p>The L1 coefficient.</p> <code>0.001</code> <code>lp_norm</code> <code>float</code> <p>The Lp norm.</p> <code>1</code> <code>scale_sparsity_penalty_by_decoder_norm</code> <code>bool</code> <p>Whether to scale the sparsity penalty by the decoder norm.</p> <code>False</code> <code>l1_warm_up_steps</code> <code>int</code> <p>The number of warm-up steps for the L1 loss.</p> <code>0</code> <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.0003</code> <code>lr_scheduler_name</code> <code>str</code> <p>The name of the learning rate scheduler to use.</p> <code>'constant'</code> <code>lr_warm_up_steps</code> <code>int</code> <p>The number of warm-up steps for the learning rate.</p> <code>0</code> <code>lr_end</code> <code>float</code> <p>The end learning rate for the cosine annealing scheduler.</p> <code>None</code> <code>lr_decay_steps</code> <code>int</code> <p>The number of decay steps for the learning rate.</p> <code>0</code> <code>n_restart_cycles</code> <code>int</code> <p>The number of restart cycles for the cosine annealing warm restarts scheduler.</p> <code>1</code> <code>finetuning_method</code> <code>str</code> <p>The method to use for finetuning.</p> <code>None</code> <code>use_ghost_grads</code> <code>bool</code> <p>Whether to use ghost gradients.</p> <code>False</code> <code>feature_sampling_window</code> <code>int</code> <p>The feature sampling window.</p> <code>2000</code> <code>dead_feature_window</code> <code>int</code> <p>The dead feature window.</p> <code>1000</code> <code>dead_feature_threshold</code> <code>float</code> <p>The dead feature threshold.</p> <code>1e-08</code> <code>n_eval_batches</code> <code>int</code> <p>The number of evaluation batches.</p> <code>10</code> <code>eval_batch_size_prompts</code> <code>int</code> <p>The batch size for evaluation.</p> <code>None</code> <code>log_to_wandb</code> <code>bool</code> <p>Whether to log to Weights &amp; Biases.</p> <code>True</code> <code>log_activations_store_to_wandb</code> <code>bool</code> <p>NOT CURRENTLY USED. Whether to log the activations store to Weights &amp; Biases.</p> <code>False</code> <code>log_optimizer_state_to_wandb</code> <code>bool</code> <p>NOT CURRENTLY USED. Whether to log the optimizer state to Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str</code> <p>The Weights &amp; Biases project to log to.</p> <code>'mats_sae_training_language_model'</code> <code>wandb_id</code> <code>str</code> <p>The Weights &amp; Biases ID.</p> <code>None</code> <code>run_name</code> <code>str</code> <p>The name of the run.</p> <code>None</code> <code>wandb_entity</code> <code>str</code> <p>The Weights &amp; Biases entity.</p> <code>None</code> <code>wandb_log_frequency</code> <code>int</code> <p>The frequency to log to Weights &amp; Biases.</p> <code>10</code> <code>eval_every_n_wandb_logs</code> <code>int</code> <p>The frequency to evaluate.</p> <code>100</code> <code>resume</code> <code>bool</code> <p>Whether to resume training.</p> <code>False</code> <code>n_checkpoints</code> <code>int</code> <p>The number of checkpoints.</p> <code>0</code> <code>checkpoint_path</code> <code>str</code> <p>The path to save checkpoints.</p> <code>'checkpoints'</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose output.</p> <code>True</code> <code>model_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments for the model.</p> <code>dict()</code> <code>model_from_pretrained_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments for the model from pretrained.</p> <code>dict()</code> Source code in <code>sae_lens/config.py</code> <pre><code>@dataclass\nclass LanguageModelSAERunnerConfig:\n    \"\"\"\n    Configuration for training a sparse autoencoder on a language model.\n\n    Args:\n        model_name (str): The name of the model to use. This should be the name of the model in the Hugging Face model hub.\n        model_class_name (str): The name of the class of the model to use. This should be either `HookedTransformer` or `HookedMamba`.\n        hook_name (str): The name of the hook to use. This should be a valid TransformerLens hook.\n        hook_eval (str): NOT CURRENTLY IN USE. The name of the hook to use for evaluation.\n        hook_layer (int): The index of the layer to hook. Used to stop forward passes early and speed up processing.\n        hook_head_index (int, optional): When the hook if for an activatio with a head index, we can specify a specific head to use here.\n        dataset_path (str): A Hugging Face dataset path.\n        dataset_trust_remote_code (bool): Whether to trust remote code when loading datasets from Huggingface.\n        streaming (bool): Whether to stream the dataset. Streaming large datasets is usually practical.\n        is_dataset_tokenized (bool): NOT IN USE. We used to use this but now automatically detect if the dataset is tokenized.\n        context_size (int): The context size to use when generating activations on which to train the SAE.\n        use_cached_activations (bool): Whether to use cached activations. This is useful when doing sweeps over the same activations.\n        cached_activations_path (str, optional): The path to the cached activations.\n        d_in (int): The input dimension of the SAE.\n        d_sae (int, optional): The output dimension of the SAE. If None, defaults to `d_in * expansion_factor`.\n        b_dec_init_method (str): The method to use to initialize the decoder bias. Zeros is likely fine.\n        expansion_factor (int): The expansion factor. Larger is better but more computationally expensive.\n        activation_fn (str): The activation function to use. Relu is standard.\n        normalize_sae_decoder (bool): Whether to normalize the SAE decoder. Unit normed decoder weights used to be preferred.\n        noise_scale (float): Using noise to induce sparsity is supported but not recommended.\n        from_pretrained_path (str, optional): The path to a pretrained SAE. We can finetune an existing SAE if needed.\n        apply_b_dec_to_input (bool): Whether to apply the decoder bias to the input. Not currently advised.\n        decoder_orthogonal_init (bool): Whether to use orthogonal initialization for the decoder. Not currently advised.\n        decoder_heuristic_init (bool): Whether to use heuristic initialization for the decoder. See Anthropic April Update.\n        init_encoder_as_decoder_transpose (bool): Whether to initialize the encoder as the transpose of the decoder. See Anthropic April Update.\n        n_batches_in_buffer (int): The number of batches in the buffer. When not using cached activations, a buffer in ram is used. The larger it is, the better shuffled the activations will be.\n        training_tokens (int): The number of training tokens.\n        finetuning_tokens (int): The number of finetuning tokens. See [here](https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes)\n        store_batch_size_prompts (int): The batch size for storing activations. This controls how many prompts are in the batch of the language model when generating actiations.\n        train_batch_size_tokens (int): The batch size for training. This controls the batch size of the SAE Training loop.\n        normalize_activations (str): Activation Normalization Strategy. Either none, expected_average_only_in (estimate the average activation norm and divide activations by it -&gt; this can be folded post training and set to None), or constant_norm_rescale (at runtime set activation norm to sqrt(d_in) and then scale up the SAE output).\n        device (str): The device to use. Usually cuda.\n        act_store_device (str): The device to use for the activation store. CPU is advised in order to save vram.\n        seed (int): The seed to use.\n        dtype (str): The data type to use.\n        prepend_bos (bool): Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.\n        autocast (bool): Whether to use autocast during training. Saves vram.\n        autocast_lm (bool): Whether to use autocast during activation fetching.\n        compile_llm (bool): Whether to compile the LLM.\n        llm_compilation_mode (str): The compilation mode to use for the LLM.\n        compile_sae (bool): Whether to compile the SAE.\n        sae_compilation_mode (str): The compilation mode to use for the SAE.\n        train_batch_size_tokens (int): The batch size for training.\n        adam_beta1 (float): The beta1 parameter for Adam.\n        adam_beta2 (float): The beta2 parameter for Adam.\n        mse_loss_normalization (str): The normalization to use for the MSE loss.\n        l1_coefficient (float): The L1 coefficient.\n        lp_norm (float): The Lp norm.\n        scale_sparsity_penalty_by_decoder_norm (bool): Whether to scale the sparsity penalty by the decoder norm.\n        l1_warm_up_steps (int): The number of warm-up steps for the L1 loss.\n        lr (float): The learning rate.\n        lr_scheduler_name (str): The name of the learning rate scheduler to use.\n        lr_warm_up_steps (int): The number of warm-up steps for the learning rate.\n        lr_end (float): The end learning rate for the cosine annealing scheduler.\n        lr_decay_steps (int): The number of decay steps for the learning rate.\n        n_restart_cycles (int): The number of restart cycles for the cosine annealing warm restarts scheduler.\n        finetuning_method (str): The method to use for finetuning.\n        use_ghost_grads (bool): Whether to use ghost gradients.\n        feature_sampling_window (int): The feature sampling window.\n        dead_feature_window (int): The dead feature window.\n        dead_feature_threshold (float): The dead feature threshold.\n        n_eval_batches (int): The number of evaluation batches.\n        eval_batch_size_prompts (int): The batch size for evaluation.\n        log_to_wandb (bool): Whether to log to Weights &amp; Biases.\n        log_activations_store_to_wandb (bool): NOT CURRENTLY USED. Whether to log the activations store to Weights &amp; Biases.\n        log_optimizer_state_to_wandb (bool): NOT CURRENTLY USED. Whether to log the optimizer state to Weights &amp; Biases.\n        wandb_project (str): The Weights &amp; Biases project to log to.\n        wandb_id (str): The Weights &amp; Biases ID.\n        run_name (str): The name of the run.\n        wandb_entity (str): The Weights &amp; Biases entity.\n        wandb_log_frequency (int): The frequency to log to Weights &amp; Biases.\n        eval_every_n_wandb_logs (int): The frequency to evaluate.\n        resume (bool): Whether to resume training.\n        n_checkpoints (int): The number of checkpoints.\n        checkpoint_path (str): The path to save checkpoints.\n        verbose (bool): Whether to print verbose output.\n        model_kwargs (dict[str, Any]): Additional keyword arguments for the model.\n        model_from_pretrained_kwargs (dict[str, Any]): Additional keyword arguments for the model from pretrained.\n    \"\"\"\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name: str = \"gelu-2l\"\n    model_class_name: str = \"HookedTransformer\"\n    hook_name: str = \"blocks.0.hook_mlp_out\"\n    hook_eval: str = \"NOT_IN_USE\"\n    hook_layer: int = 0\n    hook_head_index: Optional[int] = None\n    dataset_path: str = \"NeelNanda/c4-tokenized-2b\"\n    dataset_trust_remote_code: bool = True\n    streaming: bool = True\n    is_dataset_tokenized: bool = True\n    context_size: int = 128\n    use_cached_activations: bool = False\n    cached_activations_path: Optional[str] = (\n        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}\"\n    )\n\n    # SAE Parameters\n    d_in: int = 512\n    d_sae: Optional[int] = None\n    b_dec_init_method: str = \"geometric_median\"\n    expansion_factor: int = 4\n    activation_fn: str = \"relu\"  # relu, tanh-relu\n    normalize_sae_decoder: bool = True\n    noise_scale: float = 0.0\n    from_pretrained_path: Optional[str] = None\n    apply_b_dec_to_input: bool = True\n    decoder_orthogonal_init: bool = False\n    decoder_heuristic_init: bool = False\n    init_encoder_as_decoder_transpose: bool = False\n\n    # Activation Store Parameters\n    n_batches_in_buffer: int = 20\n    training_tokens: int = 2_000_000\n    finetuning_tokens: int = 0\n    store_batch_size_prompts: int = 32\n    train_batch_size_tokens: int = 4096\n    normalize_activations: str = (\n        \"none\"  # none, expected_average_only_in (Anthropic April Update), constant_norm_rescale (Anthropic Feb Update)\n    )\n\n    # Misc\n    device: str = \"cpu\"\n    act_store_device: str = \"with_model\"  # will be set by post init if with_model\n    seed: int = 42\n    dtype: str = \"float32\"  # type: ignore #\n    prepend_bos: bool = True\n\n    # Performance - see compilation section of lm_runner.py for info\n    autocast: bool = False  # autocast to autocast_dtype during training\n    autocast_lm: bool = False  # autocast lm during activation fetching\n    compile_llm: bool = False  # use torch.compile on the LLM\n    llm_compilation_mode: str | None = None  # which torch.compile mode to use\n    compile_sae: bool = False  # use torch.compile on the SAE\n    sae_compilation_mode: str | None = None\n\n    # Training Parameters\n\n    ## Batch size\n    train_batch_size_tokens: int = 4096\n\n    ## Adam\n    adam_beta1: float = 0\n    adam_beta2: float = 0.999\n\n    ## Loss Function\n    mse_loss_normalization: Optional[str] = None\n    l1_coefficient: float = 1e-3\n    lp_norm: float = 1\n    scale_sparsity_penalty_by_decoder_norm: bool = False\n    l1_warm_up_steps: int = 0\n\n    ## Learning Rate Schedule\n    lr: float = 3e-4\n    lr_scheduler_name: str = (\n        \"constant\"  # constant, cosineannealing, cosineannealingwarmrestarts\n    )\n    lr_warm_up_steps: int = 0\n    lr_end: Optional[float] = None  # only used for cosine annealing, default is lr / 10\n    lr_decay_steps: int = 0\n    n_restart_cycles: int = 1  # used only for cosineannealingwarmrestarts\n\n    ## FineTuning\n    finetuning_method: Optional[str] = None  # scale, decoder or unrotated_decoder\n\n    # Resampling protocol args\n    use_ghost_grads: bool = False  # want to change this to true on some timeline.\n    feature_sampling_window: int = 2000\n    dead_feature_window: int = 1000  # unless this window is larger feature sampling,\n\n    dead_feature_threshold: float = 1e-8\n\n    # Evals\n    n_eval_batches: int = 10\n    eval_batch_size_prompts: int | None = None  # useful if evals cause OOM\n\n    # WANDB\n    log_to_wandb: bool = True\n    log_activations_store_to_wandb: bool = False\n    log_optimizer_state_to_wandb: bool = False\n    wandb_project: str = \"mats_sae_training_language_model\"\n    wandb_id: Optional[str] = None\n    run_name: Optional[str] = None\n    wandb_entity: Optional[str] = None\n    wandb_log_frequency: int = 10\n    eval_every_n_wandb_logs: int = 100  # logs every 1000 steps.\n\n    # Misc\n    resume: bool = False\n    n_checkpoints: int = 0\n    checkpoint_path: str = \"checkpoints\"\n    verbose: bool = True\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n    model_from_pretrained_kwargs: dict[str, Any] = field(default_factory=dict)\n    sae_lens_version: str = field(default_factory=lambda: __version__)\n    sae_lens_training_version: str = field(default_factory=lambda: __version__)\n\n    def __post_init__(self):\n\n        if self.resume:\n            raise ValueError(\n                \"Resuming is no longer supported. You can finetune a trained SAE using cfg.from_pretrained path.\"\n                + \"If you want to load an SAE with resume=True in the config, please manually set resume=False in that config.\"\n            )\n\n        if self.use_cached_activations and self.cached_activations_path is None:\n            self.cached_activations_path = _default_cached_activations_path(\n                self.dataset_path,\n                self.model_name,\n                self.hook_name,\n                self.hook_head_index,\n            )\n\n        if not isinstance(self.expansion_factor, list):\n            self.d_sae = self.d_in * self.expansion_factor\n        self.tokens_per_buffer = (\n            self.train_batch_size_tokens * self.context_size * self.n_batches_in_buffer\n        )\n\n        if self.run_name is None:\n            self.run_name = f\"{self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n\n        if self.b_dec_init_method not in [\"geometric_median\", \"mean\", \"zeros\"]:\n            raise ValueError(\n                f\"b_dec_init_method must be geometric_median, mean, or zeros. Got {self.b_dec_init_method}\"\n            )\n\n        if self.normalize_sae_decoder and self.decoder_heuristic_init:\n            raise ValueError(\n                \"You can't normalize the decoder and use heuristic initialization.\"\n            )\n\n        if self.normalize_sae_decoder and self.scale_sparsity_penalty_by_decoder_norm:\n            raise ValueError(\n                \"Weighting loss by decoder norm makes no sense if you are normalizing the decoder weight norms to 1\"\n            )\n\n        # if we use decoder fine tuning, we can't be applying b_dec to the input\n        if (self.finetuning_method == \"decoder\") and (self.apply_b_dec_to_input):\n            raise ValueError(\n                \"If we are fine tuning the decoder, we can't be applying b_dec to the input.\\nSet apply_b_dec_to_input to False.\"\n            )\n\n        if self.normalize_activations not in [\n            \"none\",\n            \"expected_average_only_in\",\n            \"constant_norm_rescale\",\n        ]:\n            raise ValueError(\n                f\"normalize_activations must be none, expected_average_only_in, or constant_norm_rescale. Got {self.normalize_activations}\"\n            )\n\n        if self.act_store_device == \"with_model\":\n            self.act_store_device = self.device\n\n        if self.lr_end is None:\n            self.lr_end = self.lr / 10\n\n        unique_id = self.wandb_id\n        if unique_id is None:\n            unique_id = cast(\n                Any, wandb\n            ).util.generate_id()  # not sure why this type is erroring\n        self.checkpoint_path = f\"{self.checkpoint_path}/{unique_id}\"\n\n        if self.verbose:\n            print(\n                f\"Run name: {self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n            )\n            # Print out some useful info:\n            n_tokens_per_buffer = (\n                self.store_batch_size_prompts\n                * self.context_size\n                * self.n_batches_in_buffer\n            )\n            print(f\"n_tokens_per_buffer (millions): {n_tokens_per_buffer / 10 ** 6}\")\n            n_contexts_per_buffer = (\n                self.store_batch_size_prompts * self.n_batches_in_buffer\n            )\n            print(\n                f\"Lower bound: n_contexts_per_buffer (millions): {n_contexts_per_buffer / 10 ** 6}\"\n            )\n\n            total_training_steps = (\n                self.training_tokens + self.finetuning_tokens\n            ) // self.train_batch_size_tokens\n            print(f\"Total training steps: {total_training_steps}\")\n\n            total_wandb_updates = total_training_steps // self.wandb_log_frequency\n            print(f\"Total wandb updates: {total_wandb_updates}\")\n\n            # how many times will we sample dead neurons?\n            # assert self.dead_feature_window &lt;= self.feature_sampling_window, \"dead_feature_window must be smaller than feature_sampling_window\"\n            n_feature_window_samples = (\n                total_training_steps // self.feature_sampling_window\n            )\n            print(\n                f\"n_tokens_per_feature_sampling_window (millions): {(self.feature_sampling_window * self.context_size * self.train_batch_size_tokens) / 10 ** 6}\"\n            )\n            print(\n                f\"n_tokens_per_dead_feature_window (millions): {(self.dead_feature_window * self.context_size * self.train_batch_size_tokens) / 10 ** 6}\"\n            )\n            print(\n                f\"We will reset the sparsity calculation {n_feature_window_samples} times.\"\n            )\n            # print(\"Number tokens in dead feature calculation window: \", self.dead_feature_window * self.train_batch_size_tokens)\n            print(\n                f\"Number tokens in sparsity calculation window: {self.feature_sampling_window * self.train_batch_size_tokens:.2e}\"\n            )\n\n        if self.use_ghost_grads:\n            print(\"Using Ghost Grads.\")\n\n    @property\n    def total_training_tokens(self) -&gt; int:\n        return self.training_tokens + self.finetuning_tokens\n\n    @property\n    def total_training_steps(self) -&gt; int:\n        return self.total_training_tokens // self.train_batch_size_tokens\n\n    def get_base_sae_cfg_dict(self) -&gt; dict[str, Any]:\n        return {\n            \"d_in\": self.d_in,\n            \"d_sae\": self.d_sae,\n            \"dtype\": self.dtype,\n            \"device\": self.device,\n            \"model_name\": self.model_name,\n            \"hook_name\": self.hook_name,\n            \"hook_layer\": self.hook_layer,\n            \"hook_head_index\": self.hook_head_index,\n            \"activation_fn_str\": self.activation_fn,\n            \"apply_b_dec_to_input\": self.apply_b_dec_to_input,\n            \"context_size\": self.context_size,\n            \"prepend_bos\": self.prepend_bos,\n            \"dataset_path\": self.dataset_path,\n            \"dataset_trust_remote_code\": self.dataset_trust_remote_code,\n            \"finetuning_scaling_factor\": self.finetuning_method is not None,\n            \"sae_lens_training_version\": self.sae_lens_training_version,\n            \"normalize_activations\": self.normalize_activations,\n        }\n\n    def get_training_sae_cfg_dict(self) -&gt; dict[str, Any]:\n        return {\n            **self.get_base_sae_cfg_dict(),\n            \"l1_coefficient\": self.l1_coefficient,\n            \"lp_norm\": self.lp_norm,\n            \"use_ghost_grads\": self.use_ghost_grads,\n            \"normalize_sae_decoder\": self.normalize_sae_decoder,\n            \"noise_scale\": self.noise_scale,\n            \"decoder_orthogonal_init\": self.decoder_orthogonal_init,\n            \"mse_loss_normalization\": self.mse_loss_normalization,\n            \"decoder_heuristic_init\": self.decoder_heuristic_init,\n            \"init_encoder_as_decoder_transpose\": self.init_encoder_as_decoder_transpose,\n            \"normalize_activations\": self.normalize_activations,\n        }\n\n    def to_dict(self) -&gt; dict[str, Any]:\n\n        cfg_dict = {\n            **self.__dict__,\n            # some args may not be serializable by default\n            \"dtype\": str(self.dtype),\n            \"device\": str(self.device),\n            \"act_store_device\": str(self.act_store_device),\n        }\n\n        return cfg_dict\n\n    def to_json(self, path: str) -&gt; None:\n\n        if not os.path.exists(os.path.dirname(path)):\n            os.makedirs(os.path.dirname(path))\n\n        with open(path + \"cfg.json\", \"w\") as f:\n            json.dump(self.to_dict(), f, indent=2)\n\n    @classmethod\n    def from_json(cls, path: str) -&gt; \"LanguageModelSAERunnerConfig\":\n        with open(path + \"cfg.json\", \"r\") as f:\n            cfg = json.load(f)\n        return cls(**cfg)\n</code></pre>"},{"location":"api/#sae_lens.PretokenizeRunner","title":"<code>PretokenizeRunner</code>","text":"<p>Runner to pretokenize a dataset using a given tokenizer, and optionally upload to Huggingface.</p> Source code in <code>sae_lens/pretokenize_runner.py</code> <pre><code>class PretokenizeRunner:\n    \"\"\"\n    Runner to pretokenize a dataset using a given tokenizer, and optionally upload to Huggingface.\n    \"\"\"\n\n    def __init__(self, cfg: PretokenizeRunnerConfig):\n        self.cfg = cfg\n\n    def run(self):\n        \"\"\"\n        Load the dataset, tokenize it, and save it to disk and/or upload to Huggingface.\n        \"\"\"\n        dataset = load_dataset(\n            self.cfg.dataset_path,\n            data_dir=self.cfg.data_dir,\n            data_files=self.cfg.data_files,\n            split=self.cfg.split,\n            streaming=self.cfg.streaming,\n        )\n        if isinstance(dataset, DatasetDict):\n            raise ValueError(\n                \"Dataset has multiple splits. Must provide a 'split' param.\"\n            )\n        tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer_name)\n        tokenizer.model_max_length = sys.maxsize\n        tokenized_dataset = pretokenize_dataset(\n            cast(Dataset, dataset), tokenizer, self.cfg\n        )\n\n        if self.cfg.save_path is not None:\n            tokenized_dataset.save_to_disk(self.cfg.save_path)\n            metadata = metadata_from_config(self.cfg)\n            metadata_path = Path(self.cfg.save_path) / \"sae_lens.json\"\n            with open(metadata_path, \"w\") as f:\n                json.dump(metadata.__dict__, f, indent=2, ensure_ascii=False)\n\n        if self.cfg.hf_repo_id is not None:\n            push_to_hugging_face_hub(tokenized_dataset, self.cfg)\n\n        return tokenized_dataset\n</code></pre>"},{"location":"api/#sae_lens.PretokenizeRunner.run","title":"<code>run()</code>","text":"<p>Load the dataset, tokenize it, and save it to disk and/or upload to Huggingface.</p> Source code in <code>sae_lens/pretokenize_runner.py</code> <pre><code>def run(self):\n    \"\"\"\n    Load the dataset, tokenize it, and save it to disk and/or upload to Huggingface.\n    \"\"\"\n    dataset = load_dataset(\n        self.cfg.dataset_path,\n        data_dir=self.cfg.data_dir,\n        data_files=self.cfg.data_files,\n        split=self.cfg.split,\n        streaming=self.cfg.streaming,\n    )\n    if isinstance(dataset, DatasetDict):\n        raise ValueError(\n            \"Dataset has multiple splits. Must provide a 'split' param.\"\n        )\n    tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer_name)\n    tokenizer.model_max_length = sys.maxsize\n    tokenized_dataset = pretokenize_dataset(\n        cast(Dataset, dataset), tokenizer, self.cfg\n    )\n\n    if self.cfg.save_path is not None:\n        tokenized_dataset.save_to_disk(self.cfg.save_path)\n        metadata = metadata_from_config(self.cfg)\n        metadata_path = Path(self.cfg.save_path) / \"sae_lens.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata.__dict__, f, indent=2, ensure_ascii=False)\n\n    if self.cfg.hf_repo_id is not None:\n        push_to_hugging_face_hub(tokenized_dataset, self.cfg)\n\n    return tokenized_dataset\n</code></pre>"},{"location":"api/#sae_lens.SAE","title":"<code>SAE</code>","text":"<p>             Bases: <code>HookedRootModule</code></p> <p>Core Sparse Autoencoder (SAE) class used for inference. For training, see <code>TrainingSAE</code>.</p> Source code in <code>sae_lens/sae.py</code> <pre><code>class SAE(HookedRootModule):\n    \"\"\"\n    Core Sparse Autoencoder (SAE) class used for inference. For training, see `TrainingSAE`.\n    \"\"\"\n\n    cfg: SAEConfig\n    dtype: torch.dtype\n    device: torch.device\n\n    # analysis\n    use_error_term: bool\n\n    def __init__(\n        self,\n        cfg: SAEConfig,\n        use_error_term: bool = False,\n    ):\n        super().__init__()\n\n        self.cfg = cfg\n        self.activation_fn = get_activation_fn(cfg.activation_fn_str)\n        self.dtype = DTYPE_MAP[cfg.dtype]\n        self.device = torch.device(cfg.device)\n        self.use_error_term = use_error_term\n\n        self.initialize_weights_basic()\n\n        # handle presence / absence of scaling factor.\n        if self.cfg.finetuning_scaling_factor:\n            self.apply_finetuning_scaling_factor = (\n                lambda x: x * self.finetuning_scaling_factor\n            )\n        else:\n            self.apply_finetuning_scaling_factor = lambda x: x\n\n        # set up hooks\n        self.hook_sae_input = HookPoint()\n        self.hook_sae_acts_pre = HookPoint()\n        self.hook_sae_acts_post = HookPoint()\n        self.hook_sae_output = HookPoint()\n        self.hook_sae_recons = HookPoint()\n        self.hook_sae_error = HookPoint()\n\n        # handle hook_z reshaping if needed.\n        # this is very cursed and should be refactored. it exists so that we can reshape out\n        # the z activations for hook_z SAEs. but don't know d_head if we split up the forward pass\n        # into a separate encode and decode function.\n        # this will cause errors if we call decode before encode.\n        if self.cfg.hook_name.endswith(\"_z\"):\n            self.turn_on_forward_pass_hook_z_reshaping()\n        else:\n            # need to default the reshape fns\n            self.turn_off_forward_pass_hook_z_reshaping()\n\n        # handle run time activation normalization if needed:\n        if self.cfg.normalize_activations == \"constant_norm_rescale\":\n\n            #  we need to scale the norm of the input and store the scaling factor\n            def run_time_activation_norm_fn_in(x: torch.Tensor) -&gt; torch.Tensor:\n                self.x_norm_coeff = (self.cfg.d_in**0.5) / x.norm(dim=-1, keepdim=True)\n                x = x * self.x_norm_coeff\n                return x\n\n            def run_time_activation_norm_fn_out(x: torch.Tensor) -&gt; torch.Tensor:\n                x = x / self.x_norm_coeff\n                del self.x_norm_coeff  # prevents reusing\n                return x\n\n            self.run_time_activation_norm_fn_in = run_time_activation_norm_fn_in\n            self.run_time_activation_norm_fn_out = run_time_activation_norm_fn_out\n\n        else:\n            self.run_time_activation_norm_fn_in = lambda x: x\n            self.run_time_activation_norm_fn_out = lambda x: x\n\n        self.setup()  # Required for `HookedRootModule`s\n\n    def initialize_weights_basic(self):\n\n        # no config changes encoder bias init for now.\n        self.b_enc = nn.Parameter(\n            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n        # Start with the default init strategy:\n        self.W_dec = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(\n                    self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device\n                )\n            )\n        )\n\n        self.W_enc = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(\n                    self.cfg.d_in, self.cfg.d_sae, dtype=self.dtype, device=self.device\n                )\n            )\n        )\n\n        # methdods which change b_dec as a function of the dataset are implemented after init.\n        self.b_dec = nn.Parameter(\n            torch.zeros(self.cfg.d_in, dtype=self.dtype, device=self.device)\n        )\n\n        # scaling factor for fine-tuning (not to be used in initial training)\n        # TODO: Make this optional and not included with all SAEs by default (but maintain backwards compatibility)\n        if self.cfg.finetuning_scaling_factor:\n            self.finetuning_scaling_factor = nn.Parameter(\n                torch.ones(self.cfg.d_sae, dtype=self.dtype, device=self.device)\n            )\n\n    # Basic Forward Pass Functionality.\n    def forward(\n        self,\n        x: torch.Tensor,\n    ) -&gt; torch.Tensor:\n\n        feature_acts = self.encode(x)\n        sae_out = self.decode(feature_acts)\n\n        if self.use_error_term:\n            with torch.no_grad():\n                # Recompute everything without hooks to get true error term\n                # Otherwise, the output with error term will always equal input, even for causal interventions that affect x_reconstruct\n                # This is in a no_grad context to detach the error, so we can compute SAE feature gradients (eg for attribution patching). See A.3 in https://arxiv.org/pdf/2403.19647.pdf for more detail\n                # NOTE: we can't just use `sae_error = input - x_reconstruct.detach()` or something simpler, since this would mean intervening on features would mean ablating features still results in perfect reconstruction.\n\n                # move x to correct dtype\n                x = x.to(self.dtype)\n\n                # handle hook z reshaping if needed.\n                sae_in = self.reshape_fn_in(x)  # type: ignore\n\n                # handle run time activation normalization if needed\n                sae_in = self.run_time_activation_norm_fn_in(sae_in)\n\n                # apply b_dec_to_input if using that method.\n                sae_in_cent = sae_in - (self.b_dec * self.cfg.apply_b_dec_to_input)\n\n                # \"... d_in, d_in d_sae -&gt; ... d_sae\",\n                hidden_pre = sae_in_cent @ self.W_enc + self.b_enc\n                feature_acts = self.activation_fn(hidden_pre)\n                x_reconstruct_clean = self.reshape_fn_out(\n                    self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec\n                    + self.b_dec,\n                    d_head=self.d_head,\n                )\n\n                sae_out = self.run_time_activation_norm_fn_out(sae_out)\n                sae_error = self.hook_sae_error(x - x_reconstruct_clean)\n\n            return self.hook_sae_output(sae_out + sae_error)\n\n        return self.hook_sae_output(sae_out)\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"\n        Calcuate SAE features from inputs\n        \"\"\"\n\n        # move x to correct dtype\n        x = x.to(self.dtype)\n\n        # handle hook z reshaping if needed.\n        x = self.reshape_fn_in(x)  # type: ignore\n\n        # handle run time activation normalization if needed\n        x = self.run_time_activation_norm_fn_in(x)\n\n        # apply b_dec_to_input if using that method.\n        sae_in = self.hook_sae_input(x - (self.b_dec * self.cfg.apply_b_dec_to_input))\n\n        # \"... d_in, d_in d_sae -&gt; ... d_sae\",\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n        feature_acts = self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n\n        return feature_acts\n\n    def decode(\n        self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n        \"\"\"Decodes SAE feature activation tensor into a reconstructed input activation tensor.\"\"\"\n        # \"... d_sae, d_sae d_in -&gt; ... d_in\",\n        sae_out = self.hook_sae_recons(\n            self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec + self.b_dec\n        )\n\n        # handle run time activation normalization if needed\n        # will fail if you call this twice without calling encode in between.\n        sae_out = self.run_time_activation_norm_fn_out(sae_out)\n\n        # handle hook z reshaping if needed.\n        sae_out = self.reshape_fn_out(sae_out, self.d_head)  # type: ignore\n\n        return sae_out\n\n    @torch.no_grad()\n    def fold_W_dec_norm(self):\n        W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)\n        self.W_dec.data = self.W_dec.data / W_dec_norms\n        self.W_enc.data = self.W_enc.data * W_dec_norms.T\n        self.b_enc.data = self.b_enc.data * W_dec_norms.squeeze()\n\n    @torch.no_grad()\n    def fold_activation_norm_scaling_factor(\n        self, activation_norm_scaling_factor: float\n    ):\n        self.W_enc.data = self.W_enc.data * activation_norm_scaling_factor\n\n    def save_model(self, path: str, sparsity: Optional[torch.Tensor] = None):\n\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n        # generate the weights\n        save_file(self.state_dict(), f\"{path}/{SAE_WEIGHTS_PATH}\")\n\n        # save the config\n        config = self.cfg.to_dict()\n\n        with open(f\"{path}/{SAE_CFG_PATH}\", \"w\") as f:\n            json.dump(config, f)\n\n        if sparsity is not None:\n            sparsity_in_dict = {\"sparsity\": sparsity}\n            save_file(sparsity_in_dict, f\"{path}/{SPARSITY_PATH}\")  # type: ignore\n\n    @classmethod\n    def load_from_pretrained(\n        cls, path: str, device: str = \"cpu\", dtype: str = \"float32\"\n    ) -&gt; \"SAE\":\n\n        config_path = os.path.join(path, \"cfg.json\")\n        weight_path = os.path.join(path, \"sae_weights.safetensors\")\n\n        cfg_dict, state_dict, _ = load_pretrained_sae_lens_sae_components(\n            config_path, weight_path, device, dtype\n        )\n\n        sae_cfg = SAEConfig.from_dict(cfg_dict)\n\n        sae = cls(sae_cfg)\n        sae.load_state_dict(state_dict)\n\n        return sae\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        release: str,\n        sae_id: str,\n        device: str = \"cpu\",\n    ) -&gt; Tuple[\"SAE\", dict[str, Any], Optional[torch.Tensor]]:\n        \"\"\"\n\n        Load a pretrained SAE from the Hugging Face model hub.\n\n        Args:\n            release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n            id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n            device: The device to load the SAE on.\n            return_sparsity_if_present: If True, will return the log sparsity tensor if it is present in the model directory in the Hugging Face model hub.\n        \"\"\"\n\n        # get sae directory\n        sae_directory = get_pretrained_saes_directory()\n\n        # get the repo id and path to the SAE\n        if release not in sae_directory:\n            raise ValueError(\n                f\"Release {release} not found in pretrained SAEs directory.\"\n            )\n        if sae_id not in sae_directory[release].saes_map:\n            raise ValueError(f\"ID {sae_id} not found in release {release}.\")\n        sae_info = sae_directory[release]\n        hf_repo_id = sae_info.repo_id\n        hf_path = sae_info.saes_map[sae_id]\n\n        conversion_loader_name = sae_info.conversion_func or \"sae_lens\"\n        if conversion_loader_name not in NAMED_PRETRAINED_SAE_LOADERS:\n            raise ValueError(\n                f\"Conversion func {conversion_loader_name} not found in NAMED_PRETRAINED_SAE_LOADERS.\"\n            )\n        conversion_loader = NAMED_PRETRAINED_SAE_LOADERS[conversion_loader_name]\n\n        cfg_dict, state_dict, log_sparsities = conversion_loader(\n            repo_id=hf_repo_id,\n            folder_name=hf_path,\n            device=device,\n            force_download=False,\n        )\n\n        sae = cls(SAEConfig.from_dict(cfg_dict))\n        sae.load_state_dict(state_dict)\n\n        return sae, cfg_dict, log_sparsities\n\n    def get_name(self):\n        sae_name = f\"sae_{self.cfg.model_name}_{self.cfg.hook_name}_{self.cfg.d_sae}\"\n        return sae_name\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"SAE\":\n        return cls(SAEConfig.from_dict(config_dict))\n\n    def turn_on_forward_pass_hook_z_reshaping(self):\n\n        assert self.cfg.hook_name.endswith(\n            \"_z\"\n        ), \"This method should only be called for hook_z SAEs.\"\n\n        def reshape_fn_in(x: torch.Tensor):\n            self.d_head = x.shape[-1]  # type: ignore\n            self.reshape_fn_in = lambda x: einops.rearrange(\n                x, \"... n_heads d_head -&gt; ... (n_heads d_head)\"\n            )\n            return einops.rearrange(x, \"... n_heads d_head -&gt; ... (n_heads d_head)\")\n\n        self.reshape_fn_in = reshape_fn_in\n\n        self.reshape_fn_out = lambda x, d_head: einops.rearrange(\n            x, \"... (n_heads d_head) -&gt; ... n_heads d_head\", d_head=d_head\n        )\n        self.hook_z_reshaping_mode = True\n\n    def turn_off_forward_pass_hook_z_reshaping(self):\n        self.reshape_fn_in = lambda x: x\n        self.reshape_fn_out = lambda x, d_head: x\n        self.d_head = None\n        self.hook_z_reshaping_mode = False\n</code></pre>"},{"location":"api/#sae_lens.SAE.decode","title":"<code>decode(feature_acts)</code>","text":"<p>Decodes SAE feature activation tensor into a reconstructed input activation tensor.</p> Source code in <code>sae_lens/sae.py</code> <pre><code>def decode(\n    self, feature_acts: Float[torch.Tensor, \"... d_sae\"]\n) -&gt; Float[torch.Tensor, \"... d_in\"]:\n    \"\"\"Decodes SAE feature activation tensor into a reconstructed input activation tensor.\"\"\"\n    # \"... d_sae, d_sae d_in -&gt; ... d_in\",\n    sae_out = self.hook_sae_recons(\n        self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec + self.b_dec\n    )\n\n    # handle run time activation normalization if needed\n    # will fail if you call this twice without calling encode in between.\n    sae_out = self.run_time_activation_norm_fn_out(sae_out)\n\n    # handle hook z reshaping if needed.\n    sae_out = self.reshape_fn_out(sae_out, self.d_head)  # type: ignore\n\n    return sae_out\n</code></pre>"},{"location":"api/#sae_lens.SAE.encode","title":"<code>encode(x)</code>","text":"<p>Calcuate SAE features from inputs</p> Source code in <code>sae_lens/sae.py</code> <pre><code>def encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"\n    Calcuate SAE features from inputs\n    \"\"\"\n\n    # move x to correct dtype\n    x = x.to(self.dtype)\n\n    # handle hook z reshaping if needed.\n    x = self.reshape_fn_in(x)  # type: ignore\n\n    # handle run time activation normalization if needed\n    x = self.run_time_activation_norm_fn_in(x)\n\n    # apply b_dec_to_input if using that method.\n    sae_in = self.hook_sae_input(x - (self.b_dec * self.cfg.apply_b_dec_to_input))\n\n    # \"... d_in, d_in d_sae -&gt; ... d_sae\",\n    hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n    feature_acts = self.hook_sae_acts_post(self.activation_fn(hidden_pre))\n\n    return feature_acts\n</code></pre>"},{"location":"api/#sae_lens.SAE.from_pretrained","title":"<code>from_pretrained(release, sae_id, device='cpu')</code>  <code>classmethod</code>","text":"<p>Load a pretrained SAE from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>release</code> <code>str</code> <p>The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.</p> required <code>id</code> <p>The id of the SAE to load. This will be mapped to a path in the huggingface repo.</p> required <code>device</code> <code>str</code> <p>The device to load the SAE on.</p> <code>'cpu'</code> <code>return_sparsity_if_present</code> <p>If True, will return the log sparsity tensor if it is present in the model directory in the Hugging Face model hub.</p> required Source code in <code>sae_lens/sae.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    release: str,\n    sae_id: str,\n    device: str = \"cpu\",\n) -&gt; Tuple[\"SAE\", dict[str, Any], Optional[torch.Tensor]]:\n    \"\"\"\n\n    Load a pretrained SAE from the Hugging Face model hub.\n\n    Args:\n        release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.\n        id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.\n        device: The device to load the SAE on.\n        return_sparsity_if_present: If True, will return the log sparsity tensor if it is present in the model directory in the Hugging Face model hub.\n    \"\"\"\n\n    # get sae directory\n    sae_directory = get_pretrained_saes_directory()\n\n    # get the repo id and path to the SAE\n    if release not in sae_directory:\n        raise ValueError(\n            f\"Release {release} not found in pretrained SAEs directory.\"\n        )\n    if sae_id not in sae_directory[release].saes_map:\n        raise ValueError(f\"ID {sae_id} not found in release {release}.\")\n    sae_info = sae_directory[release]\n    hf_repo_id = sae_info.repo_id\n    hf_path = sae_info.saes_map[sae_id]\n\n    conversion_loader_name = sae_info.conversion_func or \"sae_lens\"\n    if conversion_loader_name not in NAMED_PRETRAINED_SAE_LOADERS:\n        raise ValueError(\n            f\"Conversion func {conversion_loader_name} not found in NAMED_PRETRAINED_SAE_LOADERS.\"\n        )\n    conversion_loader = NAMED_PRETRAINED_SAE_LOADERS[conversion_loader_name]\n\n    cfg_dict, state_dict, log_sparsities = conversion_loader(\n        repo_id=hf_repo_id,\n        folder_name=hf_path,\n        device=device,\n        force_download=False,\n    )\n\n    sae = cls(SAEConfig.from_dict(cfg_dict))\n    sae.load_state_dict(state_dict)\n\n    return sae, cfg_dict, log_sparsities\n</code></pre>"},{"location":"api/#sae_lens.SAETrainingRunner","title":"<code>SAETrainingRunner</code>","text":"<p>Class to run the training of a Sparse Autoencoder (SAE) on a TransformerLens model.</p> Source code in <code>sae_lens/sae_training_runner.py</code> <pre><code>class SAETrainingRunner:\n    \"\"\"\n    Class to run the training of a Sparse Autoencoder (SAE) on a TransformerLens model.\n    \"\"\"\n\n    cfg: LanguageModelSAERunnerConfig\n    model: HookedRootModule\n    sae: TrainingSAE\n    activations_store: ActivationsStore\n\n    def __init__(self, cfg: LanguageModelSAERunnerConfig):\n        self.cfg = cfg\n\n        self.model = load_model(\n            self.cfg.model_class_name,\n            self.cfg.model_name,\n            device=self.cfg.device,\n            model_from_pretrained_kwargs=self.cfg.model_from_pretrained_kwargs,\n        )\n\n        self.activations_store = ActivationsStore.from_config(\n            self.model,\n            self.cfg,\n        )\n\n        if self.cfg.from_pretrained_path is not None:\n            self.sae = TrainingSAE.load_from_pretrained(\n                self.cfg.from_pretrained_path, self.cfg.device\n            )\n        else:\n            self.sae = TrainingSAE(\n                TrainingSAEConfig.from_dict(\n                    self.cfg.get_training_sae_cfg_dict(),\n                )\n            )\n            self._init_sae_group_b_decs()\n\n    def run(self):\n        \"\"\"\n        Run the training of the SAE.\n        \"\"\"\n\n        if self.cfg.log_to_wandb:\n            wandb.init(\n                project=self.cfg.wandb_project,\n                config=cast(Any, self.cfg),\n                name=self.cfg.run_name,\n                id=self.cfg.wandb_id,\n            )\n\n        trainer = SAETrainer(\n            model=self.model,\n            sae=self.sae,\n            activation_store=self.activations_store,\n            save_checkpoint_fn=self.save_checkpoint,\n            cfg=self.cfg,\n        )\n\n        self._compile_if_needed()\n        sae = self.run_trainer_with_interruption_handling(trainer)\n\n        if self.cfg.log_to_wandb:\n            wandb.finish()\n\n        return sae\n\n    def _compile_if_needed(self):\n\n        # Compile model and SAE\n        #  torch.compile can provide significant speedups (10-20% in testing)\n        # using max-autotune gives the best speedups but:\n        # (a) increases VRAM usage,\n        # (b) can't be used on both SAE and LM (some issue with cudagraphs), and\n        # (c) takes some time to compile\n        # optimal settings seem to be:\n        # use max-autotune on SAE and max-autotune-no-cudagraphs on LM\n        # (also pylance seems to really hate this)\n        if self.cfg.compile_llm:\n            self.model = torch.compile(\n                self.model,\n                mode=self.cfg.llm_compilation_mode,\n            )  # type: ignore\n\n        if self.cfg.compile_sae:\n            if self.cfg.device == \"mps\":\n                backend = \"aot_eager\"\n            else:\n                backend = \"inductor\"\n\n            self.sae.training_forward_pass = torch.compile(  # type: ignore\n                self.sae.training_forward_pass,\n                mode=self.cfg.sae_compilation_mode,\n                backend=backend,\n            )  # type: ignore\n\n    def run_trainer_with_interruption_handling(self, trainer: SAETrainer):\n        try:\n            # signal handlers (if preempted)\n            signal.signal(signal.SIGINT, interrupt_callback)\n            signal.signal(signal.SIGTERM, interrupt_callback)\n\n            # train SAE\n            sae = trainer.fit()\n\n        except (KeyboardInterrupt, InterruptedException):\n            print(\"interrupted, saving progress\")\n            checkpoint_name = trainer.n_training_tokens\n            self.save_checkpoint(trainer, checkpoint_name=checkpoint_name)\n            print(\"done saving\")\n            raise\n\n        return sae\n\n    # TODO: move this into the SAE trainer or Training SAE class\n    def _init_sae_group_b_decs(\n        self,\n    ) -&gt; None:\n        \"\"\"\n        extract all activations at a certain layer and use for sae b_dec initialization\n        \"\"\"\n\n        if self.cfg.b_dec_init_method == \"geometric_median\":\n            layer_acts = self.activations_store.storage_buffer.detach()[:, 0, :]\n            # get geometric median of the activations if we're using those.\n            median = compute_geometric_median(\n                layer_acts,\n                maxiter=100,\n            ).median\n            self.sae.initialize_b_dec_with_precalculated(median)  # type: ignore\n        elif self.cfg.b_dec_init_method == \"mean\":\n            layer_acts = self.activations_store.storage_buffer.detach().cpu()[:, 0, :]\n            self.sae.initialize_b_dec_with_mean(layer_acts)  # type: ignore\n\n    def save_checkpoint(\n        self,\n        trainer: SAETrainer,\n        checkpoint_name: int | str,\n        wandb_aliases: list[str] | None = None,\n    ) -&gt; str:\n\n        checkpoint_path = f\"{trainer.cfg.checkpoint_path}/{checkpoint_name}\"\n\n        os.makedirs(checkpoint_path, exist_ok=True)\n\n        path = f\"{checkpoint_path}\"\n        os.makedirs(path, exist_ok=True)\n\n        if self.sae.cfg.normalize_sae_decoder:\n            self.sae.set_decoder_norm_to_unit_norm()\n        self.sae.save_model(path)\n\n        # let's over write the cfg file with the trainer cfg, which is a super set of the original cfg.\n        # and should not cause issues but give us more info about SAEs we trained in SAE Lens.\n        config = trainer.cfg.to_dict()\n        with open(f\"{path}/cfg.json\", \"w\") as f:\n            json.dump(config, f)\n\n        log_feature_sparsities = {\"sparsity\": trainer.log_feature_sparsity}\n\n        log_feature_sparsity_path = f\"{path}/{SPARSITY_PATH}\"\n        save_file(log_feature_sparsities, log_feature_sparsity_path)\n\n        if trainer.cfg.log_to_wandb and os.path.exists(log_feature_sparsity_path):\n            model_artifact = wandb.Artifact(\n                f\"{self.sae.get_name()}\",\n                type=\"model\",\n                metadata=dict(trainer.cfg.__dict__),\n            )\n\n            model_artifact.add_file(f\"{path}/{SAE_WEIGHTS_PATH}\")\n            model_artifact.add_file(f\"{path}/{SAE_CFG_PATH}\")\n\n            wandb.log_artifact(model_artifact, aliases=wandb_aliases)\n\n            sparsity_artifact = wandb.Artifact(\n                f\"{self.sae.get_name()}_log_feature_sparsity\",\n                type=\"log_feature_sparsity\",\n                metadata=dict(trainer.cfg.__dict__),\n            )\n            sparsity_artifact.add_file(log_feature_sparsity_path)\n            wandb.log_artifact(sparsity_artifact)\n\n        return checkpoint_path\n</code></pre>"},{"location":"api/#sae_lens.SAETrainingRunner.run","title":"<code>run()</code>","text":"<p>Run the training of the SAE.</p> Source code in <code>sae_lens/sae_training_runner.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the training of the SAE.\n    \"\"\"\n\n    if self.cfg.log_to_wandb:\n        wandb.init(\n            project=self.cfg.wandb_project,\n            config=cast(Any, self.cfg),\n            name=self.cfg.run_name,\n            id=self.cfg.wandb_id,\n        )\n\n    trainer = SAETrainer(\n        model=self.model,\n        sae=self.sae,\n        activation_store=self.activations_store,\n        save_checkpoint_fn=self.save_checkpoint,\n        cfg=self.cfg,\n    )\n\n    self._compile_if_needed()\n    sae = self.run_trainer_with_interruption_handling(trainer)\n\n    if self.cfg.log_to_wandb:\n        wandb.finish()\n\n    return sae\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE","title":"<code>TrainingSAE</code>","text":"<p>             Bases: <code>SAE</code></p> <p>A SAE used for training. This class provides a <code>training_forward_pass</code> method which calculates losses used for training.</p> Source code in <code>sae_lens/training/training_sae.py</code> <pre><code>class TrainingSAE(SAE):\n    \"\"\"\n    A SAE used for training. This class provides a `training_forward_pass` method which calculates\n    losses used for training.\n    \"\"\"\n\n    cfg: TrainingSAEConfig\n    use_error_term: bool\n    dtype: torch.dtype\n    device: torch.device\n\n    def __init__(self, cfg: TrainingSAEConfig, use_error_term: bool = False):\n\n        base_sae_cfg = SAEConfig.from_dict(cfg.get_base_sae_cfg_dict())\n        super().__init__(base_sae_cfg)\n        self.cfg = cfg  # type: ignore\n        self.use_error_term = use_error_term\n\n        self.initialize_weights_complex()\n\n        # The training SAE will assume that the activation store handles\n        # reshaping.\n        self.turn_off_forward_pass_hook_z_reshaping()\n\n        self.mse_loss_fn = self._get_mse_loss_fn()\n\n    @classmethod\n    def from_dict(cls, config_dict: dict[str, Any]) -&gt; \"TrainingSAE\":\n        return cls(TrainingSAEConfig.from_dict(config_dict))\n\n    def encode(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n        \"\"\"\n        Calcuate SAE features from inputs\n        \"\"\"\n        feature_acts, _ = self.encode_with_hidden_pre(x)\n        return feature_acts\n\n    def encode_with_hidden_pre(\n        self, x: Float[torch.Tensor, \"... d_in\"]\n    ) -&gt; tuple[Float[torch.Tensor, \"... d_sae\"], Float[torch.Tensor, \"... d_sae\"]]:\n\n        # move x to correct dtype\n        x = x.to(self.dtype)\n\n        # handle hook z reshaping if needed.\n        x = self.reshape_fn_in(x)  # type: ignore\n\n        # apply b_dec_to_input if using that method.\n        sae_in = self.hook_sae_input(x - (self.b_dec * self.cfg.apply_b_dec_to_input))\n\n        # handle run time activation normalization if needed\n        x = self.run_time_activation_norm_fn_in(x)\n\n        # \"... d_in, d_in d_sae -&gt; ... d_sae\",\n        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)\n        hidden_pre_noised = hidden_pre + (\n            torch.randn_like(hidden_pre) * self.cfg.noise_scale * self.training\n        )\n        feature_acts = self.hook_sae_acts_post(self.activation_fn(hidden_pre_noised))\n\n        return feature_acts, hidden_pre_noised\n\n    def forward(\n        self,\n        x: Float[torch.Tensor, \"... d_in\"],\n    ) -&gt; Float[torch.Tensor, \"... d_in\"]:\n\n        feature_acts, _ = self.encode_with_hidden_pre(x)\n        sae_out = self.decode(feature_acts)\n\n        return sae_out\n\n    def training_forward_pass(\n        self,\n        sae_in: torch.Tensor,\n        current_l1_coefficient: float,\n        dead_neuron_mask: Optional[torch.Tensor] = None,\n    ) -&gt; TrainStepOutput:\n\n        # do a forward pass to get SAE out, but we also need the\n        # hidden pre.\n        feature_acts, _ = self.encode_with_hidden_pre(sae_in)\n        sae_out = self.decode(feature_acts)\n\n        # MSE LOSS\n        per_item_mse_loss = self.mse_loss_fn(sae_out, sae_in)\n        mse_loss = per_item_mse_loss.sum(dim=-1).mean()\n\n        # GHOST GRADS\n        if self.cfg.use_ghost_grads and self.training and dead_neuron_mask is not None:\n\n            # first half of second forward pass\n            _, hidden_pre = self.encode_with_hidden_pre(sae_in)\n            ghost_grad_loss = self.calculate_ghost_grad_loss(\n                x=sae_in,\n                sae_out=sae_out,\n                per_item_mse_loss=per_item_mse_loss,\n                hidden_pre=hidden_pre,\n                dead_neuron_mask=dead_neuron_mask,\n            )\n        else:\n            ghost_grad_loss = 0.0\n\n        # SPARSITY LOSS\n        # either the W_dec norms are 1 and this won't do anything or they are not 1\n        # and we're using their norm in the loss function.\n        weighted_feature_acts = feature_acts * self.W_dec.norm(dim=1)\n        sparsity = weighted_feature_acts.norm(\n            p=self.cfg.lp_norm, dim=-1\n        )  # sum over the feature dimension\n\n        l1_loss = (current_l1_coefficient * sparsity).mean()\n\n        loss = mse_loss + l1_loss + ghost_grad_loss\n\n        return TrainStepOutput(\n            sae_in=sae_in,\n            sae_out=sae_out,\n            feature_acts=feature_acts,\n            loss=loss,\n            mse_loss=mse_loss.item(),\n            l1_loss=l1_loss.item(),\n            ghost_grad_loss=(\n                ghost_grad_loss.item()\n                if isinstance(ghost_grad_loss, torch.Tensor)\n                else ghost_grad_loss\n            ),\n        )\n\n    def calculate_ghost_grad_loss(\n        self,\n        x: torch.Tensor,\n        sae_out: torch.Tensor,\n        per_item_mse_loss: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        dead_neuron_mask: torch.Tensor,\n    ) -&gt; torch.Tensor:\n\n        # 1.\n        residual = x - sae_out\n        l2_norm_residual = torch.norm(residual, dim=-1)\n\n        # 2.\n        # ghost grads use an exponentional activation function, ignoring whatever\n        # the activation function is in the SAE. The forward pass uses the dead neurons only.\n        feature_acts_dead_neurons_only = torch.exp(hidden_pre[:, dead_neuron_mask])\n        ghost_out = feature_acts_dead_neurons_only @ self.W_dec[dead_neuron_mask, :]\n        l2_norm_ghost_out = torch.norm(ghost_out, dim=-1)\n        norm_scaling_factor = l2_norm_residual / (1e-6 + l2_norm_ghost_out * 2)\n        ghost_out = ghost_out * norm_scaling_factor[:, None].detach()\n\n        # 3. There is some fairly complex rescaling here to make sure that the loss\n        # is comparable to the original loss. This is because the ghost grads are\n        # only calculated for the dead neurons, so we need to rescale the loss to\n        # make sure that the loss is comparable to the original loss.\n        # There have been methodological improvements that are not implemented here yet\n        # see here: https://www.lesswrong.com/posts/C5KAZQib3bzzpeyrg/full-post-progress-update-1-from-the-gdm-mech-interp-team#Improving_ghost_grads\n        per_item_mse_loss_ghost_resid = self.mse_loss_fn(ghost_out, residual.detach())\n        mse_rescaling_factor = (\n            per_item_mse_loss / (per_item_mse_loss_ghost_resid + 1e-6)\n        ).detach()\n        per_item_mse_loss_ghost_resid = (\n            mse_rescaling_factor * per_item_mse_loss_ghost_resid\n        )\n\n        return per_item_mse_loss_ghost_resid.mean()\n\n    @torch.no_grad()\n    def _get_mse_loss_fn(self) -&gt; Any:\n\n        def standard_mse_loss_fn(\n            preds: torch.Tensor, target: torch.Tensor\n        ) -&gt; torch.Tensor:\n            return torch.nn.functional.mse_loss(preds, target, reduction=\"none\")\n\n        def batch_norm_mse_loss_fn(\n            preds: torch.Tensor, target: torch.Tensor\n        ) -&gt; torch.Tensor:\n            target_centered = target - target.mean(dim=0, keepdim=True)\n            normalization = target_centered.norm(dim=-1, keepdim=True)\n            return torch.nn.functional.mse_loss(preds, target, reduction=\"none\") / (\n                normalization + 1e-6\n            )\n\n        if self.cfg.mse_loss_normalization == \"dense_batch\":\n            return batch_norm_mse_loss_fn\n        else:\n            return standard_mse_loss_fn\n\n    @classmethod\n    def load_from_pretrained(\n        cls,\n        path: str,\n        device: str = \"cpu\",\n        dtype: str = \"float32\",\n    ) -&gt; \"TrainingSAE\":\n\n        config_path = os.path.join(path, \"cfg.json\")\n        weight_path = os.path.join(path, \"sae_weights.safetensors\")\n\n        cfg_dict, state_dict, _ = load_pretrained_sae_lens_sae_components(\n            config_path, weight_path, device, dtype\n        )\n\n        sae_cfg = TrainingSAEConfig.from_dict(cfg_dict)\n\n        sae = cls(sae_cfg)\n        sae.load_state_dict(state_dict)\n\n        return sae\n\n    def initialize_weights_complex(self):\n        \"\"\" \"\"\"\n\n        if self.cfg.decoder_orthogonal_init:\n            self.W_dec.data = nn.init.orthogonal_(self.W_dec.data.T).T\n\n        elif self.cfg.decoder_heuristic_init:\n            self.W_dec = nn.Parameter(\n                torch.rand(\n                    self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device\n                )\n            )\n            self.initialize_decoder_norm_constant_norm()\n\n        elif self.cfg.normalize_sae_decoder:\n            self.set_decoder_norm_to_unit_norm()\n\n        # Then we intialize the encoder weights (either as the transpose of decoder or not)\n        if self.cfg.init_encoder_as_decoder_transpose:\n            self.W_enc.data = self.W_dec.data.T.clone().contiguous()\n        else:\n            self.W_enc = nn.Parameter(\n                torch.nn.init.kaiming_uniform_(\n                    torch.empty(\n                        self.cfg.d_in,\n                        self.cfg.d_sae,\n                        dtype=self.dtype,\n                        device=self.device,\n                    )\n                )\n            )\n\n        if self.cfg.normalize_sae_decoder:\n            with torch.no_grad():\n                # Anthropic normalize this to have unit columns\n                self.set_decoder_norm_to_unit_norm()\n\n    ## Initialization Methods\n    @torch.no_grad()\n    def initialize_b_dec_with_precalculated(self, origin: torch.Tensor):\n        out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n        self.b_dec.data = out\n\n    @torch.no_grad()\n    def initialize_b_dec_with_mean(self, all_activations: torch.Tensor):\n        previous_b_dec = self.b_dec.clone().cpu()\n        out = all_activations.mean(dim=0)\n\n        previous_distances = torch.norm(all_activations - previous_b_dec, dim=-1)\n        distances = torch.norm(all_activations - out, dim=-1)\n\n        print(\"Reinitializing b_dec with mean of activations\")\n        print(\n            f\"Previous distances: {previous_distances.median(0).values.mean().item()}\"\n        )\n        print(f\"New distances: {distances.median(0).values.mean().item()}\")\n\n        self.b_dec.data = out.to(self.dtype).to(self.device)\n\n    ## Training Utils\n    @torch.no_grad()\n    def set_decoder_norm_to_unit_norm(self):\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n\n    @torch.no_grad()\n    def initialize_decoder_norm_constant_norm(self, norm: float = 0.1):\n        \"\"\"\n        A heuristic proceedure inspired by:\n        https://transformer-circuits.pub/2024/april-update/index.html#training-saes\n        \"\"\"\n        # TODO: Parameterise this as a function of m and n\n\n        # ensure W_dec norms at unit norm\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n        self.W_dec.data *= norm  # will break tests but do this for now.\n\n    @torch.no_grad()\n    def remove_gradient_parallel_to_decoder_directions(self):\n        \"\"\"\n        Update grads so that they remove the parallel component\n            (d_sae, d_in) shape\n        \"\"\"\n        assert self.W_dec.grad is not None  # keep pyright happy\n\n        parallel_component = einops.einsum(\n            self.W_dec.grad,\n            self.W_dec.data,\n            \"d_sae d_in, d_sae d_in -&gt; d_sae\",\n        )\n        self.W_dec.grad -= einops.einsum(\n            parallel_component,\n            self.W_dec.data,\n            \"d_sae, d_sae d_in -&gt; d_sae d_in\",\n        )\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.encode","title":"<code>encode(x)</code>","text":"<p>Calcuate SAE features from inputs</p> Source code in <code>sae_lens/training/training_sae.py</code> <pre><code>def encode(\n    self, x: Float[torch.Tensor, \"... d_in\"]\n) -&gt; Float[torch.Tensor, \"... d_sae\"]:\n    \"\"\"\n    Calcuate SAE features from inputs\n    \"\"\"\n    feature_acts, _ = self.encode_with_hidden_pre(x)\n    return feature_acts\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.initialize_decoder_norm_constant_norm","title":"<code>initialize_decoder_norm_constant_norm(norm=0.1)</code>","text":"<p>A heuristic proceedure inspired by: https://transformer-circuits.pub/2024/april-update/index.html#training-saes</p> Source code in <code>sae_lens/training/training_sae.py</code> <pre><code>@torch.no_grad()\ndef initialize_decoder_norm_constant_norm(self, norm: float = 0.1):\n    \"\"\"\n    A heuristic proceedure inspired by:\n    https://transformer-circuits.pub/2024/april-update/index.html#training-saes\n    \"\"\"\n    # TODO: Parameterise this as a function of m and n\n\n    # ensure W_dec norms at unit norm\n    self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n    self.W_dec.data *= norm  # will break tests but do this for now.\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.initialize_weights_complex","title":"<code>initialize_weights_complex()</code>","text":"Source code in <code>sae_lens/training/training_sae.py</code> <pre><code>def initialize_weights_complex(self):\n    \"\"\" \"\"\"\n\n    if self.cfg.decoder_orthogonal_init:\n        self.W_dec.data = nn.init.orthogonal_(self.W_dec.data.T).T\n\n    elif self.cfg.decoder_heuristic_init:\n        self.W_dec = nn.Parameter(\n            torch.rand(\n                self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device\n            )\n        )\n        self.initialize_decoder_norm_constant_norm()\n\n    elif self.cfg.normalize_sae_decoder:\n        self.set_decoder_norm_to_unit_norm()\n\n    # Then we intialize the encoder weights (either as the transpose of decoder or not)\n    if self.cfg.init_encoder_as_decoder_transpose:\n        self.W_enc.data = self.W_dec.data.T.clone().contiguous()\n    else:\n        self.W_enc = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(\n                    self.cfg.d_in,\n                    self.cfg.d_sae,\n                    dtype=self.dtype,\n                    device=self.device,\n                )\n            )\n        )\n\n    if self.cfg.normalize_sae_decoder:\n        with torch.no_grad():\n            # Anthropic normalize this to have unit columns\n            self.set_decoder_norm_to_unit_norm()\n</code></pre>"},{"location":"api/#sae_lens.TrainingSAE.remove_gradient_parallel_to_decoder_directions","title":"<code>remove_gradient_parallel_to_decoder_directions()</code>","text":"<p>Update grads so that they remove the parallel component     (d_sae, d_in) shape</p> Source code in <code>sae_lens/training/training_sae.py</code> <pre><code>@torch.no_grad()\ndef remove_gradient_parallel_to_decoder_directions(self):\n    \"\"\"\n    Update grads so that they remove the parallel component\n        (d_sae, d_in) shape\n    \"\"\"\n    assert self.W_dec.grad is not None  # keep pyright happy\n\n    parallel_component = einops.einsum(\n        self.W_dec.grad,\n        self.W_dec.data,\n        \"d_sae d_in, d_sae d_in -&gt; d_sae\",\n    )\n    self.W_dec.grad -= einops.einsum(\n        parallel_component,\n        self.W_dec.data,\n        \"d_sae, d_sae d_in -&gt; d_sae d_in\",\n    )\n</code></pre>"},{"location":"citation/","title":"Citation","text":"<pre><code>@misc{bloom2024saetrainingcodebase,\n   title = {SAELens Training\n   author = {Joseph Bloom},\n   year = {2024},\n   howpublished = {\\url{}},\n}}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! To get setup for development, follow the instructions below.</p>"},{"location":"contributing/#setup","title":"Setup","text":"<p>Make sure you have poetry installed, clone the repository, and install dependencies with:</p> <pre><code>git clone https://github.com/jbloomAus/SAELens.git # we recommend you make a fork for submitting PR's and clone that!\npoetry lock # can take a while.\npoetry install \nmake check-ci # validate the install\n</code></pre>"},{"location":"contributing/#testing-linting-and-formatting","title":"Testing, Linting, and Formatting","text":"<p>This project uses pytest for testing, flake8 for linting, pyright for type-checking, and black and isort for formatting.</p> <p>If you add new code, it would be greatly appreciated if you could add tests in the <code>tests/unit</code> directory. You can run the tests with:</p> <pre><code>make unit-test\n</code></pre> <p>Before commiting, make sure you format the code with:</p> <pre><code>make format\n</code></pre> <p>Finally, run all CI checks locally with:</p> <pre><code>make check-ci\n</code></pre> <p>If these pass, you're good to go! Open a pull request with your changes.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>This project uses mkdocs for documentation. You can see the docs locally with:</p> <p><pre><code>make docs-serve\n</code></pre> If you make changes to code which requires updating documentation, it would be greatly appreciated if you could update the docs as well.</p>"},{"location":"feature_dashboards/","title":"Feature dashboards","text":""},{"location":"feature_dashboards/#example-output","title":"Example Output","text":"<p>Here's one feature we found in the residual stream of Layer 10 of GPT-2 Small:</p> <p>. Open <code>gpt2_resid_pre10_predict_pronoun_feature.html</code> in your browser to interact with the dashboard (WIP).</p> <p>Note, probably this feature could split into more mono-semantic features in a larger SAE that had been trained for longer. (this was was only about 49152 features trained on 10M tokens from OpenWebText).</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#motivation","title":"Motivation","text":"<ul> <li>Accelerate SAE Research: Support fast experimentation to understand SAEs and improve SAE training so we can train SAEs on larger and more diverse models.</li> <li>Make Research like Play: Support research into language model internals via SAEs. Good tooling can make research tremendously exciting and enjoyable. Balancing modifiability and reliability with ease of understanding / access is the name of the game here.</li> <li>Build an awesome community: Mechanistic Interpretability already has an awesome community but as that community grows, it makes sense that there will be niches. I'd love to build a great community around Sparse Autoencoders.</li> </ul>"},{"location":"roadmap/#goals","title":"Goals","text":""},{"location":"roadmap/#sae-training","title":"SAE Training","text":"<p>SAE Training features will fit into a number of categories including:</p> <ul> <li>Making it easy to train SAEs: Training SAEs is hard for a number of reasons and so making it easy for people to train SAEs with relatively little expertise seems like the main way this codebase will create value. </li> <li>Training SAEs on more models: Supporting training of SAEs on more models, architectures, different activations within those models.</li> <li>Being better at training SAEs: Enabling methodological changes which may improve SAE performance as measured by reconstruction loss, Cross Entropy Loss when using reconstructed activation, L1 loss, L0 and interpretability of features as well as improving speed of training or reducing the compute resources required to train SAEs. </li> <li>Being better at measuring SAE Performance: How do we know when SAEs are doing what we want them to? Improving training metrics should allow better decisions about which methods to use and which hyperparameters choices we make.</li> <li>Training SAE variants: People are already training \u201cTranscoders\u201d which map from one activation to another (such as before / after an MLP layer). These can be easily supported with a few changes. Other variants will come in time and </li> </ul>"},{"location":"roadmap/#analysis-with-saes","title":"Analysis with SAEs","text":"<p>Using SAEs to understand neural network internals is an exciting, but complicated task.</p> <ul> <li>Feature-wise Interpretability: This looks something like \"for each feature, have as much knowledge about it as possible\". Part of this will feature dashboard improvements, or supporting better integrations with Neuronpedia.</li> <li>Mechanistic Interpretability: This comprises the more traditional kinds of Mechanistic Interpretability which TransformerLens supports and should be supported by this codebase. Making it easy to patch, ablate or otherwise intervene on features so as to find circuits will likely speed up lots of researchers.</li> </ul>"},{"location":"roadmap/#other-stuff","title":"Other Stuff","text":"<p>I think there are lots of other types of analysis that could be done in the future with SAE features. I've already explored many different types of statistical tests which can reveal interesting properties of features. There are also things like saliency mapping and attribution techniques which it would be nice to support.</p> <ul> <li>Accessibility and Code Quality: The codebase won\u2019t be used if it doesn\u2019t work and it also won\u2019t get used if it\u2019s too hard to understand, modify or read.  Making the code accessible: This involves tasks like turning the code base into a python package.</li> <li>Knowing how the code is supposed to work: Is the code well-documented? This will require docstrings, tutorials and links to related work and publications. Getting aligned on what the code does is critical to sharing a resource like this. </li> <li>Knowing the code works as intended: All code should be tested. Unit tests and acceptance tests are both important.</li> <li>Knowing the code is actually performant: This will ensure code works as intended. However deep learning introduces lots of complexity which makes actually running benchmarks essential to having confidence in the code. </li> </ul>"},{"location":"training_saes/","title":"Training Sparse Autoencoders","text":"<p>Methods development for training SAEs is rapidly evolving, so these docs may change frequently. For all available training options, see LanguageModelSAERunnerConfig.</p> <p>However, we are attempting to maintain this tutorial .</p> <p>We encourage readers to join the Open Source Mechanistic Interpretability Slack for support!</p>"},{"location":"training_saes/#basic-training-setup","title":"Basic training setup","text":"<p>Training a SAE is done using the SAETrainingRunner class. This class is configured using a LanguageModelSAERunnerConfig, and has a single method, run(), which performs training.</p> <p>Some of the core config options are below:</p> <ul> <li><code>model_name</code>: The base model name to train a SAE on. This must correspond to a model from TransformerLens.</li> <li><code>hook_name</code>: This is a TransformerLens hook in the model where our SAE will be trained from. More info on hooks can be found here.</li> <li><code>dataset_path</code>: The path to a dataset on Huggingface for training.</li> <li><code>hook_layer</code>: This is an int which corresponds to the layer specified in <code>hook_name</code>. This must match! e.g. if <code>hook_name</code> is <code>\"blocks.3.hook_mlp_out\"</code>, then <code>layer</code> must be <code>3</code>.</li> <li><code>d_in</code>: The input size of the SAE. This must match the size of the hook in the model where the SAE is trained.</li> <li><code>expansion_factor</code>: The hidden layer of the SAE will have size <code>expansion_factor * d_in</code>.</li> <li><code>l1_coefficient</code>: This controls how much sparsity the SAE will have after training.</li> <li><code>training_tokens</code>: The total tokens used for training.</li> <li><code>train_batch_size_tokens</code>: The batch size used for training. Adjust this to keep the GPU saturated.</li> </ul> <p>A sample training run from the tutorial is shown below:</p> <pre><code>total_training_steps = 30_000\nbatch_size = 4096\ntotal_training_tokens = total_training_steps * batch_size\n\nlr_warm_up_steps = 0\nlr_decay_steps = total_training_steps // 5  # 20% of training\nl1_warm_up_steps = total_training_steps // 20  # 5% of training\n\ncfg = LanguageModelSAERunnerConfig(\n    # Data Generating Function (Model + Training Distibuion)\n    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n    hook_name=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n    hook_layer=0,  # Only one layer in the model.\n    d_in=1024,  # the width of the mlp output.\n    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n    is_dataset_tokenized=True,\n    streaming=True,  # we could pre-download the token dataset if it was small.\n    # SAE Parameters\n    mse_loss_normalization=None,  # We won't normalize the mse loss,\n    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n    normalize_sae_decoder=False,\n    scale_sparsity_penalty_by_decoder_norm=True,\n    decoder_heuristic_init=True,\n    init_encoder_as_decoder_transpose=True,\n    normalize_activations=\"expected_average_only_in\",\n    # Training Parameters\n    lr=5e-5,\n    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n    adam_beta2=0.999,\n    lr_scheduler_name=\"constant\",  # constant learning rate with warmup.\n    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n    l1_coefficient=5,  # will control how sparse the feature activations are\n    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n    lp_norm=1.0,  # the L1 penalty (and not a Lp for p &lt; 1)\n    train_batch_size_tokens=batch_size,\n    context_size=256,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n    # Activation Store Parameters\n    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n    store_batch_size_prompts=16,\n    # Resampling protocol\n    use_ghost_grads=False,  # we don't use ghost grads anymore.\n    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n    # WANDB\n    log_to_wandb=True,  # always use wandb unless you are just testing code.\n    wandb_project=\"sae_lens_tutorial\",\n    wandb_log_frequency=30,\n    eval_every_n_wandb_logs=20,\n    # Misc\n    device=device,\n    seed=42,\n    n_checkpoints=0,\n    checkpoint_path=\"checkpoints\",\n    dtype=\"float32\"\n)\nsparse_autoencoder = SAETrainingRunner(cfg).run()\n</code></pre> <p>As you can see, the training setup provides a large number of options to explore. The full list of options can be found in the LanguageModelSAERunnerConfig class.</p>"},{"location":"training_saes/#logging-to-weights-and-biases","title":"Logging to Weights and Biases","text":"<p>For any real training run, you should be logging to Weights and Biases (WandB). This will allow you to track your training progress and compare different runs. To enable WandB, set <code>log_to_wandb=True</code>. The <code>wandb_project</code> parameter in the config controls the project name in WandB. You can also control the logging frequency with <code>wandb_log_frequency</code> and <code>eval_every_n_wandb_logs</code>.</p> <p>A number of helpful metrics are logged to WandB, including the sparsity of the SAE, the mean squared error (MSE) of the SAE, dead features, and explained variance. These metrics can be used to monitor the training progress and adjust the training parameters. Below is a screenshot from one training run. </p> <p></p>"},{"location":"training_saes/#checkpoints","title":"Checkpoints","text":"<p>Checkpoints allow you to save a snapshot of the SAE and sparsitity statistics during training. To enable checkpointing, set <code>n_checkpoints</code> to a value larger than 0. If WandB logging is enabled, checkpoints will be uploaded as WandB artifacts. To save checkpoints locally, the <code>checkpoint_path</code> parameter can be set to a local directory.</p>"},{"location":"training_saes/#optimizers-and-schedulers","title":"Optimizers and Schedulers","text":"<p>The SAE training runner uses the Adam optimizer with a constant learning rate by default. The optimizer betas can be controlled with the settings <code>adam_beta1</code> and <code>adam_beta2</code>.</p> <p>The learning rate scheduler can be controlled with the <code>lr_scheduler_name</code> parameter. The available schedulers are: <code>constant</code> (default), <code>consineannealing</code>, and <code>cosineannealingwarmrestarts</code>. All schedulers can be used with linear warmup and linear decay, set via <code>lr_warm_up_steps</code> and <code>lr_decay_steps</code>.</p> <p>To avoid dead features, it's often helpful to slowly increase the L1 penalty. This can be done by setting <code>l1_warm_up_steps</code> to a value larger than 0. This will linearly increase the L1 penalty over the first <code>l1_warm_up_steps</code> training steps.</p>"},{"location":"training_saes/#datasets-streaming-and-context-size","title":"Datasets, streaming, and context size","text":"<p>SAELens works with datasets hosted on Huggingface. However, these datsets are often very large and take a long time and a lot of disk space to download. To speed this up, you can set <code>streaming=True</code> in the config. This will stream the dataset from Huggingface during training, which will allow training to start immediately and save disk space.</p> <p>The <code>context_size</code> parameter controls the length of the prompts fed to the model. Larger context sizes will result in better SAE performance, but will also slow down training. Each training batch will be tokens of size <code>train_batch_size_tokens x context_size</code>.</p> <p>It's also possible to use pre-tokenized datasets to speed up training, since tokenization can be a bottleneck. To use a pre-tokenized dataset on Huggingface, update the <code>dataset_path</code> parameter and set <code>is_dataset_tokenized=True</code> in the config.</p>"},{"location":"training_saes/#pretokenizing-datasets","title":"Pretokenizing datasets","text":"<p>We also provider a runner, PretokenizeRunner, which can be used to pre-tokenize a dataset and upload it to Huggingface. See PretokenizeRunnerConfig for all available options. We also provide a pretokenizing datasets tutorial with more details.</p> <p>A sample run from the tutorial for GPT2 and the NeelNanda/c4-10k dataset is shown below.</p> <pre><code>from sae_lens import PretokenizeRunner, PretokenizeRunnerConfig\n\ncfg = PretokenizeRunnerConfig(\n    tokenizer_name=\"gpt2\",\n    dataset_path=\"NeelNanda/c4-10k\", # this is just a tiny test dataset\n    shuffle=True,\n    num_proc=4, # increase this number depending on how many CPUs you have\n\n    # tweak these settings depending on the model\n    context_size=128,\n    begin_batch_token=\"bos\",\n    begin_sequence_token=None,\n    sequence_separator_token=\"eos\",\n\n    # uncomment to upload to huggingface\n    # hf_repo_id=\"your-username/c4-10k-tokenized-gpt2\"\n\n    # uncomment to save the dataset locally\n    # save_path=\"./c4-10k-tokenized-gpt2\"\n)\n\ndataset = PretokenizeRunner(cfg).run()\n</code></pre>"},{"location":"training_saes/#caching-activations","title":"Caching activations","text":"<p>The next step in improving performance beyond pre-tokenizing datasets is to cache model activations. This allows you to pre-calculate all the training activations for your SAE in advance so the model does not need to be run during training to generate activations. This allows rapid training of SAEs and is especially helpful for experimenting with training hyperparameters. However, pre-calculating activations can take a very large amount of disk space, so it may not always be possible.</p> <p>SAELens provides a CacheActivationsRunner class to help with pre-calculating activations. See CacheActivationsRunnerConfig for all available options. This runner intentionally shares a lot of options with LanguageModelSAERunnerConfig. These options should be set identically when using the cached activations in training. The CacheActivationsRunner can be used as below:</p> <pre><code>from sae_lens import CacheActivationsRunner, CacheActivationsRunnerConfig\n\ncfg = CacheActivationsRunnerConfig(\n    model_name=\"tiny-stories-1L-21M\",\n    hook_name=\"blocks.0.hook_mlp_out\",\n    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n    # ... \n    new_cached_activations_path=\"./tiny-stories-1L-21M-cache\"\n)\n\nCacheActivationsRunner(cfg).run()\n</code></pre> <p>To use the cached activations during training, set <code>use_cached_activations=True</code> and <code>cached_activations_path</code> to match the <code>new_cached_activations_path</code> above option in training configuration.</p>"}]}